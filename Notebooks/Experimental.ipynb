{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 125981,
          "databundleVersionId": 14910697,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31234,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebookf7ce7c4fa0",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "Sd2HSYK8qMBK"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "the_blind_flight_synapse_drive_ps_1_path = kagglehub.competition_download('the-blind-flight-synapse-drive-ps-1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "vT51ZlxnqMBN"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Source - https://stackoverflow.com/a\n",
        "# Posted by Shaima' safaaldin Bahaaldin, modified by community. See post 'Timeline' for change history\n",
        "# Retrieved 2025-12-20, License - CC BY-SA 4.0\n",
        "\n",
        "\n",
        "!pip install \"numpy<2\"\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "0ufrczjQqMBP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def order_points(pts):\n",
        "    \"\"\" Standard corner ordering: TL, TR, BR, BL \"\"\"\n",
        "    rect = np.zeros((4, 2), dtype=\"float32\")\n",
        "    s = pts.sum(axis=1)\n",
        "    rect[0] = pts[np.argmin(s)]\n",
        "    rect[2] = pts[np.argmax(s)]\n",
        "    diff = np.diff(pts, axis=1)\n",
        "    rect[1] = pts[np.argmin(diff)]\n",
        "    rect[3] = pts[np.argmax(diff)]\n",
        "    return rect\n",
        "\n",
        "def intelligent_grid_slice(image_path, target_size=800):\n",
        "    img = cv2.imread(str(image_path))\n",
        "    if img is None: return None, None, \"Error\"\n",
        "\n",
        "    orig = img.copy()\n",
        "    h, w = img.shape[:2]\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # 1. Edge Detection (Lower thresholds to catch faint lines)\n",
        "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                   cv2.THRESH_BINARY_INV, 11, 2)\n",
        "\n",
        "    # 2. Find Contours\n",
        "    cnts, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
        "\n",
        "    grid_contour = None\n",
        "\n",
        "    for c in cnts:\n",
        "        peri = cv2.arcLength(c, True)\n",
        "        approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
        "\n",
        "        # We look for a 4-sided polygon that is LARGE (at least 50% of image area)\n",
        "        # This prevents picking up a small square inside the board\n",
        "        if len(approx) == 4 and cv2.contourArea(c) > (h * w * 0.5):\n",
        "            grid_contour = approx\n",
        "            break\n",
        "\n",
        "    # 3. DECISION LOGIC\n",
        "    if grid_contour is not None:\n",
        "        # CASE A: Corners are visible (Standard Skewed Image)\n",
        "        rect = order_points(grid_contour.reshape(4, 2))\n",
        "\n",
        "        dst = np.array([\n",
        "            [0, 0],\n",
        "            [target_size - 1, 0],\n",
        "            [target_size - 1, target_size - 1],\n",
        "            [0, target_size - 1]], dtype=\"float32\")\n",
        "\n",
        "        M = cv2.getPerspectiveTransform(rect, dst)\n",
        "        warped = cv2.warpPerspective(orig, M, (target_size, target_size))\n",
        "        mode = \"Contour De-Skew\"\n",
        "\n",
        "    else:\n",
        "        # CASE B: No Corners found (Zoomed In / Overfitted Board)\n",
        "        # We assume the image IS the grid. Just resize.\n",
        "        warped = cv2.resize(orig, (target_size, target_size))\n",
        "        mode = \"Full Image Resize\"\n",
        "\n",
        "    # 4. SLICING\n",
        "    cells = []\n",
        "    rows, cols = 20, 20\n",
        "    cell_h = target_size // rows\n",
        "    cell_w = target_size // cols\n",
        "\n",
        "    for y in range(rows):\n",
        "        for x in range(cols):\n",
        "            y1, y2 = y * cell_h, (y + 1) * cell_h\n",
        "            x1, x2 = x * cell_w, (x + 1) * cell_w\n",
        "\n",
        "            cell = warped[y1:y2, x1:x2]\n",
        "\n",
        "            # Center Crop (Crucial to remove grid lines)\n",
        "            crop = 4\n",
        "            if cell.shape[0] > 2*crop and cell.shape[1] > 2*crop:\n",
        "                cell = cell[crop:-crop, crop:-crop]\n",
        "\n",
        "            cells.append(cell)\n",
        "\n",
        "    return warped, np.array(cells), mode\n",
        "\n",
        "# --- VISUALIZATION ---\n",
        "# Replace with your test image path to verify\n",
        "# warped, cells, mode = intelligent_grid_slice(\"test_image.png\")\n",
        "# print(f\"Processing Mode: {mode}\")\n",
        "# plt.imshow(warped)\n",
        "def visualize_processing(image_path):\n",
        "    # 1. Run the intelligent slicer\n",
        "    warped, cells, mode = intelligent_grid_slice(image_path)\n",
        "\n",
        "    if warped is None:\n",
        "        print(\"❌ Error processing image\")\n",
        "        return\n",
        "\n",
        "    # 2. Setup Plot\n",
        "    fig = plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # --- Subplot 1: Original Image ---\n",
        "    ax1 = fig.add_subplot(2, 2, 1)\n",
        "    orig_img = cv2.imread(str(image_path))\n",
        "    ax1.imshow(cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB))\n",
        "    ax1.set_title(\"Original Image\", fontsize=14, fontweight='bold')\n",
        "    ax1.axis('off')\n",
        "\n",
        "    # --- Subplot 2: The \"Warped/Flat\" Result ---\n",
        "    ax2 = fig.add_subplot(2, 2, 2)\n",
        "    ax2.imshow(cv2.cvtColor(warped, cv2.COLOR_BGR2RGB))\n",
        "    ax2.set_title(f\"Processed Board ({mode})\", fontsize=14, fontweight='bold', color='green')\n",
        "    ax2.axis('off')\n",
        "\n",
        "    # --- Subplot 3: The 400 Grid Cells ---\n",
        "    # We create a 20x20 grid of subplots in the bottom half\n",
        "    print(f\"✅ Sliced into {len(cells)} cells. Displaying grid...\")\n",
        "\n",
        "    # Create a nested gridspec for the bottom half\n",
        "    import matplotlib.gridspec as gridspec\n",
        "    gs = gridspec.GridSpec(2, 1, height_ratios=[1, 2]) # Top row (images), Bottom row (grid)\n",
        "\n",
        "    # We already plotted top row, now let's do the bottom massive grid\n",
        "    # To make it easier, let's just use a new figure logic for the grid or stitch them\n",
        "    # Actually, simpler approach: Stitch cells back together with white borders for display\n",
        "\n",
        "    rows, cols = 20, 20\n",
        "    cell_h, cell_w, _ = cells[0].shape\n",
        "\n",
        "    # Create a canvas with gaps between cells\n",
        "    gap = 2\n",
        "    grid_canvas_h = rows * cell_h + (rows + 1) * gap\n",
        "    grid_canvas_w = cols * cell_w + (cols + 1) * gap\n",
        "    grid_canvas = np.ones((grid_canvas_h, grid_canvas_w, 3), dtype=np.uint8) * 255 # White background\n",
        "\n",
        "    idx = 0\n",
        "    for y in range(rows):\n",
        "        for x in range(cols):\n",
        "            c_img = cells[idx]\n",
        "            # Calculate placement\n",
        "            y_start = gap + y * (cell_h + gap)\n",
        "            x_start = gap + x * (cell_w + gap)\n",
        "\n",
        "            # Place cell\n",
        "            grid_canvas[y_start:y_start+cell_h, x_start:x_start+cell_w] = c_img\n",
        "            idx += 1\n",
        "\n",
        "    ax3 = fig.add_subplot(2, 1, 2)\n",
        "    ax3.imshow(cv2.cvtColor(grid_canvas, cv2.COLOR_BGR2RGB))\n",
        "    ax3.set_title(\"Final 20x20 Sliced Grid (With padding)\", fontsize=14, fontweight='bold')\n",
        "    ax3.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- USAGE ---\n",
        "# Replace with a real path to test\n",
        "TEST_IMG = \"/kaggle/input/the-blind-flight-synapse-drive-ps-1/SynapseDrive_Dataset/test/images/0004.png\"\n",
        "\n",
        "visualize_processing(TEST_IMG)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "tCcTum2xqMBR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def order_points(pts):\n",
        "    \"\"\"\n",
        "    Sorts 4 points: Top-Left, Top-Right, Bottom-Right, Bottom-Left\n",
        "    \"\"\"\n",
        "    rect = np.zeros((4, 2), dtype=\"float32\")\n",
        "\n",
        "    # Top-Left has smallest sum(x+y), Bottom-Right has largest sum(x+y)\n",
        "    s = pts.sum(axis=1)\n",
        "    rect[0] = pts[np.argmin(s)]\n",
        "    rect[2] = pts[np.argmax(s)]\n",
        "\n",
        "    # Top-Right has smallest diff(y-x), Bottom-Left has largest diff(y-x)\n",
        "    diff = np.diff(pts, axis=1)\n",
        "    rect[1] = pts[np.argmin(diff)]\n",
        "    rect[3] = pts[np.argmax(diff)]\n",
        "    return rect\n",
        "\n",
        "def align_by_corners(image_path):\n",
        "    # 1. Load Image\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None: return None, None, \"Error\"\n",
        "    orig = img.copy()\n",
        "\n",
        "    # 2. Isolate Grid Lines (Morphological Trick)\n",
        "    # This removes all terrain, leaving only the \"mesh\"\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                   cv2.THRESH_BINARY_INV, 15, 5)\n",
        "\n",
        "    # Kernels to extract lines\n",
        "    scale = 20\n",
        "    h_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (scale, 1))\n",
        "    v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, scale))\n",
        "\n",
        "    # Filter out everything that isn't a long line\n",
        "    mask_h = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, h_kernel)\n",
        "    mask_v = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, v_kernel)\n",
        "    grid_mask = cv2.addWeighted(mask_h, 0.5, mask_v, 0.5, 0)\n",
        "\n",
        "    # 3. Detect Corners on the Clean Grid\n",
        "    # maxCorners=0 means \"unlimited\", qualityLevel=0.2 filters weak corners\n",
        "    corners = cv2.goodFeaturesToTrack(grid_mask, maxCorners=1000, qualityLevel=0.2, minDistance=20)\n",
        "\n",
        "    if corners is None: return None, None, \"No corners found\"\n",
        "\n",
        "    corners = np.int0(corners)\n",
        "    corner_pts = corners.reshape(-1, 2) # Flatten to list of (x,y)\n",
        "\n",
        "    # 4. Find the \"Extreme\" Corners (The bounding quadrilateral)\n",
        "    # This finds the corners farthest apart to define the board area\n",
        "    rect = order_points(corner_pts)\n",
        "    (tl, tr, br, bl) = rect\n",
        "\n",
        "    # 5. Perspective Transform (De-Skew)\n",
        "    # We map these extreme corners to a perfect square (800x800)\n",
        "    target_size = 800\n",
        "    dst = np.array([\n",
        "        [0, 0],\n",
        "        [target_size - 1, 0],\n",
        "        [target_size - 1, target_size - 1],\n",
        "        [0, target_size - 1]], dtype=\"float32\")\n",
        "\n",
        "    M = cv2.getPerspectiveTransform(rect, dst)\n",
        "    warped = cv2.warpPerspective(orig, M, (target_size, target_size))\n",
        "\n",
        "    return warped, grid_mask, \"Success\"\n",
        "\n",
        "# --- VISUALIZATION ---\n",
        "def visualize_deskew(image_path):\n",
        "    warped, mask, status = align_by_corners(image_path)\n",
        "\n",
        "    if warped is None:\n",
        "        print(status)\n",
        "        return\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    # Show Original\n",
        "    img = cv2.imread(image_path)\n",
        "    ax[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    ax[0].set_title(\"Original\")\n",
        "\n",
        "    # Show the \"Clean\" Grid Mask (What the computer sees)\n",
        "    ax[1].imshow(mask, cmap='gray')\n",
        "    ax[1].set_title(\"Morphological Grid Mask\")\n",
        "\n",
        "    # Show the De-skewed Result\n",
        "    ax[2].imshow(cv2.cvtColor(warped, cv2.COLOR_BGR2RGB))\n",
        "    ax[2].set_title(\"Aligned & Reskewed\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "visualize_deskew(\"/kaggle/input/the-blind-flight-synapse-drive-ps-1/SynapseDrive_Dataset/test/images/0009.png\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "0mgbtZ2qqMBU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_morphological_mask(img):\n",
        "    \"\"\" Reusing your morphological logic which worked perfectly \"\"\"\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                   cv2.THRESH_BINARY_INV, 15, 5)\n",
        "\n",
        "    scale = 20\n",
        "    h_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (scale, 1))\n",
        "    v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, scale))\n",
        "\n",
        "    mask_h = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, h_kernel)\n",
        "    mask_v = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, v_kernel)\n",
        "\n",
        "    # Combine to get the mesh (Grid lines = White, Cells = Black)\n",
        "    grid_mask = cv2.addWeighted(mask_h, 0.5, mask_v, 0.5, 0)\n",
        "    return grid_mask\n",
        "\n",
        "def slice_from_mask(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None: return None, None\n",
        "\n",
        "    # 1. Get the Grid \"Skeleton\"\n",
        "    grid_mask = get_morphological_mask(img)\n",
        "\n",
        "    # 2. Invert: Make Grid Lines Black (0), Cells White (255)\n",
        "    # We use a simple threshold to make sure it's binary\n",
        "    _, binary_grid = cv2.threshold(grid_mask, 50, 255, cv2.THRESH_BINARY)\n",
        "    inverted_mask = cv2.bitwise_not(binary_grid)\n",
        "\n",
        "    # 3. Find Contours (The Cells)\n",
        "    cnts, _ = cv2.findContours(inverted_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    cells = []\n",
        "    cell_coords = []\n",
        "\n",
        "    # 4. Filter and Crop\n",
        "    img_area = img.shape[0] * img.shape[1]\n",
        "    min_area = img_area / (30 * 30) # Approx smallest valid cell\n",
        "    max_area = img_area / (10 * 10) # Approx largest valid cell (prevents picking full background)\n",
        "\n",
        "    for c in cnts:\n",
        "        area = cv2.contourArea(c)\n",
        "\n",
        "        # Filter noise\n",
        "        if area > min_area and area < max_area:\n",
        "            # Get Bounding Box\n",
        "            x, y, w, h = cv2.boundingRect(c)\n",
        "\n",
        "            # Crop slightly inside the box to avoid the grid lines\n",
        "            margin = 2\n",
        "            if w > 2*margin and h > 2*margin:\n",
        "                roi = img[y+margin : y+h-margin, x+margin : x+w-margin]\n",
        "\n",
        "                # Store cell and its center (x, y) for sorting later\n",
        "                center_x = x + w // 2\n",
        "                center_y = y + h // 2\n",
        "\n",
        "                cells.append(roi)\n",
        "                cell_coords.append((center_x, center_y))\n",
        "\n",
        "    # 5. Sort Cells (Top-to-Bottom, Left-to-Right)\n",
        "    # This reconstructs the grid order roughly.\n",
        "    # Note: If cells are missing, the indices will shift, but this gives a good visual order.\n",
        "    if len(cell_coords) > 0:\n",
        "        # Zip, sort by Y (row), then roughly by X (col)\n",
        "        # Using a \"row tolerance\" to group items in the same row\n",
        "        zipped = sorted(zip(cells, cell_coords), key=lambda k: k[1][1]) # Sort by Y first\n",
        "\n",
        "        sorted_cells = []\n",
        "        # Basic heuristic: Sort fully by Y, then assuming 20 items per row is risky if some are missing.\n",
        "        # Instead, we just return the raw blobs for now to visualize.\n",
        "        sorted_cells = [x[0] for x in zipped]\n",
        "        return sorted_cells, inverted_mask\n",
        "\n",
        "    return cells, inverted_mask\n",
        "\n",
        "# --- VISUALIZATION ---\n",
        "def visualize_blob_slice(image_path):\n",
        "    cells, mask = slice_from_mask(image_path)\n",
        "\n",
        "    if not cells:\n",
        "        print(\"❌ No cells found!\")\n",
        "        return\n",
        "\n",
        "    print(f\"✅ Extracted {len(cells)} candidate cells.\")\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Show the Mask we used\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    ax1.imshow(mask, cmap='gray')\n",
        "    ax1.set_title(\"Inverted Grid Mask (White = Cell)\")\n",
        "    ax1.axis('off')\n",
        "\n",
        "    # Show the first 100 extracted cells in a grid\n",
        "    ax2 = fig.add_subplot(1, 2, 2)\n",
        "\n",
        "    # Create a canvas to stitch them for preview\n",
        "    # We'll just show the first 64 (8x8) for sanity check\n",
        "    preview_dim = int(np.sqrt(len(cells)))\n",
        "    if preview_dim > 8: preview_dim = 8\n",
        "\n",
        "    cell_h, cell_w = 32, 32 # Resize for display\n",
        "    canvas = np.zeros((preview_dim * cell_h, preview_dim * cell_w, 3), dtype=np.uint8)\n",
        "\n",
        "    idx = 0\n",
        "    for i in range(preview_dim):\n",
        "        for j in range(preview_dim):\n",
        "            if idx < len(cells):\n",
        "                resized = cv2.resize(cells[idx], (cell_w, cell_h))\n",
        "                canvas[i*cell_h:(i+1)*cell_h, j*cell_w:(j+1)*cell_w] = resized\n",
        "                idx += 1\n",
        "\n",
        "    ax2.imshow(cv2.cvtColor(canvas, cv2.COLOR_BGR2RGB))\n",
        "    ax2.set_title(f\"Sample Extracted Cells (Unsorted)\")\n",
        "    ax2.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Replace with your image\n",
        "\n",
        "visualize_blob_slice(\"/kaggle/input/the-blind-flight-synapse-drive-ps-1/SynapseDrive_Dataset/test/images/0009.png\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "zkrOBsDIqMBW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import Rbf\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "IMAGE_PATH = \"/kaggle/input/the-blind-flight-synapse-drive-ps-1/SynapseDrive_Dataset/test/images/0004.png\"  # CHANGE THIS\n",
        "GRID_N = 20\n",
        "\n",
        "# ---------------- LOAD IMAGE ----------------\n",
        "img = cv2.imread(IMAGE_PATH)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "h, w = img.shape[:2]\n",
        "\n",
        "# ---------------- EDGE + LINE DETECTION ----------------\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "edges = cv2.Canny(gray, 60, 150)\n",
        "\n",
        "lines = cv2.HoughLinesP(\n",
        "    edges,\n",
        "    rho=1,\n",
        "    theta=np.pi/180,\n",
        "    threshold=180,\n",
        "    minLineLength=250,\n",
        "    maxLineGap=20\n",
        ")\n",
        "\n",
        "if lines is None:\n",
        "    raise RuntimeError(\"No grid lines detected\")\n",
        "\n",
        "h_lines, v_lines = [], []\n",
        "for l in lines:\n",
        "    x1,y1,x2,y2 = l[0]\n",
        "    if abs(y1-y2) < abs(x1-x2):\n",
        "        h_lines.append((x1,y1,x2,y2))\n",
        "    else:\n",
        "        v_lines.append((x1,y1,x2,y2))\n",
        "\n",
        "# ---------------- INTERSECTIONS ----------------\n",
        "def intersect(l1, l2):\n",
        "    x1,y1,x2,y2 = l1\n",
        "    x3,y3,x4,y4 = l2\n",
        "\n",
        "    A = np.array([[x2-x1, x3-x4],\n",
        "                  [y2-y1, y3-y4]])\n",
        "    B = np.array([x3-x1, y3-y1])\n",
        "\n",
        "    if abs(np.linalg.det(A)) < 1e-6:\n",
        "        return None\n",
        "\n",
        "    t = np.linalg.solve(A, B)[0]\n",
        "    return x1 + t*(x2-x1), y1 + t*(y2-y1)\n",
        "\n",
        "pts = []\n",
        "for hl in h_lines:\n",
        "    for vl in v_lines:\n",
        "        p = intersect(hl, vl)\n",
        "        if p and 0 <= p[0] < w and 0 <= p[1] < h:\n",
        "            pts.append(p)\n",
        "\n",
        "pts = np.array(pts)\n",
        "print(\"Intersections:\", len(pts))\n",
        "\n",
        "# ---------------- CLUSTER POINTS INTO GRID ----------------\n",
        "K = (GRID_N+1)**2\n",
        "pts = pts[np.random.choice(len(pts), min(len(pts), K*2), replace=False)]\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=K, n_init=10, random_state=0).fit(pts)\n",
        "src = kmeans.cluster_centers_\n",
        "\n",
        "# Sort row-major\n",
        "src = src[np.lexsort((src[:,0], src[:,1]))]\n",
        "\n",
        "# ---------------- IDEAL GRID ----------------\n",
        "xs = np.linspace(0, w-1, GRID_N+1)\n",
        "ys = np.linspace(0, h-1, GRID_N+1)\n",
        "dst = np.array([(x,y) for y in ys for x in xs])\n",
        "\n",
        "# ---------------- TPS VIA RBF ----------------\n",
        "rbf_x = Rbf(src[:,0], src[:,1], dst[:,0], function=\"thin_plate\")\n",
        "rbf_y = Rbf(src[:,0], src[:,1], dst[:,1], function=\"thin_plate\")\n",
        "\n",
        "grid_x, grid_y = np.meshgrid(np.arange(w), np.arange(h))\n",
        "map_x = rbf_x(grid_x, grid_y).astype(np.float32)\n",
        "map_y = rbf_y(grid_x, grid_y).astype(np.float32)\n",
        "\n",
        "warped = cv2.remap(img, map_x, map_y, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "# ---------------- VISUALIZE ----------------\n",
        "plt.figure(figsize=(14,6))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Original (Skewed)\")\n",
        "plt.imshow(img)\n",
        "plt.scatter(src[:,0], src[:,1], s=5, c=\"red\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"TPS Rectified (SciPy)\")\n",
        "plt.imshow(warped)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "HrWNlzRQqMBZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "def preprocess_for_grid(img):\n",
        "    \"\"\"\n",
        "    Special preprocessing to kill texture (Lab/Forest) but keep black lines.\n",
        "    \"\"\"\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # 1. MEDIAN BLUR (The Fix for Lab Texture)\n",
        "    # This removes high-frequency noise (like sand/checkerboard)\n",
        "    # while preserving strong edges (grid lines).\n",
        "    no_texture = cv2.medianBlur(gray, 7)\n",
        "\n",
        "    # 2. Adaptive Threshold to find black lines\n",
        "    thresh = cv2.adaptiveThreshold(no_texture, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                   cv2.THRESH_BINARY_INV, 19, 5)\n",
        "    return thresh\n",
        "\n",
        "def get_intersections(img):\n",
        "    thresh = preprocess_for_grid(img)\n",
        "\n",
        "    # Morphological Kernels to isolate grid lines\n",
        "    scale = 25 # Slightly larger for robustness\n",
        "    h_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (scale, 1))\n",
        "    v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, scale))\n",
        "\n",
        "    mask_h = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, h_kernel)\n",
        "    mask_v = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, v_kernel)\n",
        "\n",
        "    intersections = cv2.bitwise_and(mask_h, mask_v)\n",
        "    intersections = cv2.dilate(intersections, np.ones((5,5))) # Dilate more to ensure connectivity\n",
        "\n",
        "    num_labels, _, stats, centroids = cv2.connectedComponentsWithStats(intersections)\n",
        "\n",
        "    points = []\n",
        "    for i in range(1, num_labels):\n",
        "        if stats[i, cv2.CC_STAT_AREA] > 10: # Ignore tiny noise\n",
        "            points.append(centroids[i])\n",
        "\n",
        "    if not points: return np.array([])\n",
        "\n",
        "    # Clustering to merge close points (Fixes double-detection)\n",
        "    points = np.array(points)\n",
        "    clustering = DBSCAN(eps=20, min_samples=1).fit(points)\n",
        "\n",
        "    clean_points = []\n",
        "    for label in set(clustering.labels_):\n",
        "        cluster_pts = points[clustering.labels_ == label]\n",
        "        clean_points.append(np.mean(cluster_pts, axis=0))\n",
        "\n",
        "    return np.array(clean_points)\n",
        "\n",
        "def sort_points_robust(points):\n",
        "    # Cluster Y-coordinates to define Rows\n",
        "    y_coords = points[:, 1].reshape(-1, 1)\n",
        "    y_clustering = DBSCAN(eps=25, min_samples=3).fit(y_coords)\n",
        "\n",
        "    rows_dict = {}\n",
        "    for pt, label in zip(points, y_clustering.labels_):\n",
        "        if label == -1: continue\n",
        "        if label not in rows_dict: rows_dict[label] = []\n",
        "        rows_dict[label].append(pt)\n",
        "\n",
        "    # Sort rows top-to-bottom\n",
        "    sorted_keys = sorted(rows_dict.keys(), key=lambda k: np.mean([p[1] for p in rows_dict[k]]))\n",
        "\n",
        "    final_rows = []\n",
        "    for k in sorted_keys:\n",
        "        row_pts = rows_dict[k]\n",
        "        row_pts.sort(key=lambda p: p[0]) # Sort left-to-right\n",
        "        final_rows.append(np.array(row_pts))\n",
        "\n",
        "    return final_rows\n",
        "\n",
        "def reconstruct_grid_safe(image_path, cell_size=40):\n",
        "    \"\"\"\n",
        "    Main function with FALLBACK. Never returns None.\n",
        "    \"\"\"\n",
        "    # 1. Load Safe\n",
        "    img = cv2.imread(str(image_path))\n",
        "    if img is None:\n",
        "        print(f\"❌ Error: Cannot read {image_path}\")\n",
        "        return np.zeros((800, 800, 3), dtype=np.uint8) # Return black square on fail\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "    fallback_img = cv2.resize(img, (800, 800)) # Default fallback\n",
        "\n",
        "    try:\n",
        "        # 2. Detect\n",
        "        points = get_intersections(img)\n",
        "\n",
        "        # Validation: Do we have enough points for a grid?\n",
        "        if len(points) < 50:\n",
        "            print(f\"⚠️ Low detection ({len(points)} pts). Using fallback resize.\")\n",
        "            return fallback_img\n",
        "\n",
        "        # 3. Organize\n",
        "        grid_rows = sort_points_robust(points)\n",
        "        if len(grid_rows) < 10: # Too few rows\n",
        "            return fallback_img\n",
        "\n",
        "        # 4. Warp Cells\n",
        "        num_rows = len(grid_rows)\n",
        "        # Use median row length to define width\n",
        "        row_lens = [len(r) for r in grid_rows]\n",
        "        num_cols = int(np.median(row_lens))\n",
        "\n",
        "        canvas_h = (num_rows - 1) * cell_size\n",
        "        canvas_w = (num_cols - 1) * cell_size\n",
        "\n",
        "        # Create Grey Canvas (so we can see if cells are missing)\n",
        "        final_grid = np.ones((canvas_h, canvas_w, 3), dtype=np.uint8) * 128\n",
        "\n",
        "        for r in range(num_rows - 1):\n",
        "            row_top = grid_rows[r]\n",
        "            row_btm = grid_rows[r+1]\n",
        "\n",
        "            # Simple matching: find closest X in bottom row\n",
        "            for pt_top in row_top:\n",
        "                # Find closest point in bottom row (by X coordinate)\n",
        "                # Filter to only look at points within reasonable X distance (skew limit)\n",
        "                candidates = [p for p in row_btm if abs(p[0] - pt_top[0]) < 60]\n",
        "\n",
        "                if not candidates: continue\n",
        "\n",
        "                # We need a Top-Right neighbor to form a quad\n",
        "                # Look for a point in row_top that is to the RIGHT of pt_top\n",
        "                neighbors = [p for p in row_top if p[0] > pt_top[0]]\n",
        "                if not neighbors: continue\n",
        "                pt_tr = min(neighbors, key=lambda p: p[0]) # The immediate right neighbor\n",
        "\n",
        "                if abs(pt_tr[0] - pt_top[0]) > 80: continue # Neighbor too far (gap in grid)\n",
        "\n",
        "                # Now find Bottom-Right matching that Top-Right\n",
        "                pt_bl = min(candidates, key=lambda p: abs(p[0] - pt_top[0]))\n",
        "\n",
        "                # Find BR\n",
        "                candidates_br = [p for p in row_btm if abs(p[0] - pt_tr[0]) < 60]\n",
        "                if not candidates_br: continue\n",
        "                pt_br = min(candidates_br, key=lambda p: abs(p[0] - pt_tr[0]))\n",
        "\n",
        "                # Warp\n",
        "                src = np.array([pt_top, pt_tr, pt_br, pt_bl], dtype=\"float32\")\n",
        "                dst = np.array([[0,0], [cell_size,0], [cell_size,cell_size], [0,cell_size]], dtype=\"float32\")\n",
        "\n",
        "                M = cv2.getPerspectiveTransform(src, dst)\n",
        "                warped = cv2.warpPerspective(img, M, (cell_size, cell_size))\n",
        "\n",
        "                # Determine Place\n",
        "                # Heuristic: grid index based on X coordinate relative to image width\n",
        "                # This approximates the column index even if points are missing\n",
        "                col_idx = int(pt_top[0] / (w / num_cols))\n",
        "\n",
        "                # Refine placement logic: Just stack them?\n",
        "                # Better: Use the row index 'r' and a simple counter if the grid is dense\n",
        "                # For visualization, let's just use relative position\n",
        "\n",
        "                y_pos = r * cell_size\n",
        "                # Find matching column index in the 'row_top' array\n",
        "                # (This assumes row_top is sorted, which it is)\n",
        "                c_idx = np.where(np.all(row_top == pt_top, axis=1))[0][0]\n",
        "\n",
        "                x_pos = c_idx * cell_size\n",
        "\n",
        "                if y_pos+cell_size <= canvas_h and x_pos+cell_size <= canvas_w:\n",
        "                    # Crop and paste\n",
        "                    warped = warped[2:-2, 2:-2] # Crop border\n",
        "                    warped = cv2.resize(warped, (cell_size, cell_size))\n",
        "                    final_grid[y_pos:y_pos+cell_size, x_pos:x_pos+cell_size] = warped\n",
        "\n",
        "        return final_grid\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Processing Error: {e}. Falling back.\")\n",
        "        return fallback_img\n",
        "\n",
        "# --- ERROR-FREE VISUALIZATION ---\n",
        "def visualize_safe(image_path):\n",
        "    # This now returns an IMAGE (Array) always. Never None.\n",
        "    result = reconstruct_grid_safe(image_path)\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    # Check if result is empty or weird shape\n",
        "    if result.shape[0] == 0:\n",
        "        print(\"Empty result\")\n",
        "        return\n",
        "\n",
        "    plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Grid Result (Safe Mode)\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Replace with your Lab image path\n",
        "visualize_safe(\"/kaggle/input/the-blind-flight-synapse-drive-ps-1/SynapseDrive_Dataset/test/images/0064.png\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "UWJXmBZsqMBc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import DBSCAN\n",
        "from pathlib import Path\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- CONFIG ---\n",
        "TEST_IMG_DIR = Path(\"/kaggle/input/the-blind-flight-synapse-drive-ps-1/SynapseDrive_Dataset/test/images\")\n",
        "NUM_TO_VISUALIZE = 50\n",
        "\n",
        "# ==========================================\n",
        "#    CORE GRID PROCESSING FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def preprocess_for_grid(img):\n",
        "    \"\"\" Removes texture (Lab/Forest) but keeps black lines. \"\"\"\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    # Median Blur is crucial for killing texture noise\n",
        "    no_texture = cv2.medianBlur(gray, 7)\n",
        "    thresh = cv2.adaptiveThreshold(no_texture, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                   cv2.THRESH_BINARY_INV, 19, 5)\n",
        "    return thresh\n",
        "\n",
        "def get_intersections(img):\n",
        "    thresh = preprocess_for_grid(img)\n",
        "    scale = 25\n",
        "\n",
        "    # --- CHANGE 1: THICKER KERNELS (25x3) ---\n",
        "    # Catches lines even if they are slightly tilted/skewed\n",
        "    h_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (scale, 3))\n",
        "    v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, scale))\n",
        "\n",
        "    mask_h = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, h_kernel)\n",
        "    mask_v = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, v_kernel)\n",
        "\n",
        "    intersections = cv2.bitwise_and(mask_h, mask_v)\n",
        "    intersections = cv2.dilate(intersections, np.ones((5,5)))\n",
        "\n",
        "    num_labels, _, stats, centroids = cv2.connectedComponentsWithStats(intersections)\n",
        "    points = []\n",
        "    for i in range(1, num_labels):\n",
        "        if stats[i, cv2.CC_STAT_AREA] > 10: points.append(centroids[i])\n",
        "\n",
        "    if not points: return np.array([])\n",
        "\n",
        "    # DBSCAN Clustering to merge double-detections\n",
        "    points = np.array(points)\n",
        "    clustering = DBSCAN(eps=20, min_samples=1).fit(points)\n",
        "    clean_points = []\n",
        "    for label in set(clustering.labels_):\n",
        "        clean_points.append(np.mean(points[clustering.labels_ == label], axis=0))\n",
        "    return np.array(clean_points)\n",
        "\n",
        "def sort_points_robust(points):\n",
        "    # Cluster Y-coordinates to define Rows\n",
        "    y_clustering = DBSCAN(eps=25, min_samples=3).fit(points[:, 1].reshape(-1, 1))\n",
        "    rows_dict = {}\n",
        "    for pt, label in zip(points, y_clustering.labels_):\n",
        "        if label == -1: continue\n",
        "        if label not in rows_dict: rows_dict[label] = []\n",
        "        rows_dict[label].append(pt)\n",
        "\n",
        "    # Sort rows top-to-bottom, points left-to-right\n",
        "    sorted_keys = sorted(rows_dict.keys(), key=lambda k: np.mean([p[1] for p in rows_dict[k]]))\n",
        "    final_rows = []\n",
        "    for k in sorted_keys:\n",
        "        row_pts = sorted(rows_dict[k], key=lambda p: p[0])\n",
        "        final_rows.append(np.array(row_pts))\n",
        "    return final_rows\n",
        "\n",
        "def reconstruct_grid_safe_with_status(image_path, cell_size=40):\n",
        "    \"\"\" Returns (processed_image, status_string) \"\"\"\n",
        "    img = cv2.imread(str(image_path))\n",
        "    if img is None: return np.zeros((800,800,3), dtype=np.uint8), \"Load Error\"\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "    fallback_img = cv2.resize(img, (800, 800))\n",
        "\n",
        "    try:\n",
        "        points = get_intersections(img)\n",
        "        # VALIDATION THRESHOLDS\n",
        "        if len(points) < 40: return fallback_img, f\"Fallback: Low Pts ({len(points)})\"\n",
        "\n",
        "        grid_rows = sort_points_robust(points)\n",
        "        if len(grid_rows) < 8: return fallback_img, f\"Fallback: Few Rows ({len(grid_rows)})\"\n",
        "\n",
        "        # WARPING LOOP\n",
        "        num_rows = len(grid_rows)\n",
        "        # Use median row length to estimate grid width\n",
        "        row_lens = [len(r) for r in grid_rows]\n",
        "        if not row_lens: return fallback_img, \"Fallback: Empty Rows\"\n",
        "        num_cols_est = int(np.median(row_lens))\n",
        "        if num_cols_est < 8: return fallback_img, f\"Fallback: Narrow Grid ({num_cols_est} cols)\"\n",
        "\n",
        "        canvas_h = (num_rows - 1) * cell_size\n",
        "        canvas_w = (num_cols_est - 1) * cell_size\n",
        "        # Grey background to show gaps\n",
        "        final_grid = np.ones((canvas_h, canvas_w, 3), dtype=np.uint8) * 128\n",
        "        cells_warped = 0\n",
        "\n",
        "        for r in range(num_rows - 1):\n",
        "            row_top = grid_rows[r]\n",
        "            row_btm = grid_rows[r+1]\n",
        "\n",
        "            for pt_top in row_top:\n",
        "                # 1. Find TR neighbor\n",
        "                neighbors_tr = [p for p in row_top if p[0] > pt_top[0]]\n",
        "                if not neighbors_tr: continue\n",
        "                pt_tr = min(neighbors_tr, key=lambda p: p[0])\n",
        "\n",
        "                # --- CHANGE 3: RELAXED SKEW LIMIT (120px) ---\n",
        "                if abs(pt_tr[0] - pt_top[0]) > 120: continue\n",
        "\n",
        "                # 2. Find BL match in bottom row\n",
        "                candidates_bl = [p for p in row_btm if abs(p[0] - pt_top[0]) < 70]\n",
        "                if not candidates_bl: continue\n",
        "                pt_bl = min(candidates_bl, key=lambda p: abs(p[0] - pt_top[0]))\n",
        "\n",
        "                # 3. Find BR match in bottom row\n",
        "                candidates_br = [p for p in row_btm if abs(p[0] - pt_tr[0]) < 70]\n",
        "                if not candidates_br: continue\n",
        "                pt_br = min(candidates_br, key=lambda p: abs(p[0] - pt_tr[0]))\n",
        "\n",
        "                # Warp\n",
        "                src = np.array([pt_top, pt_tr, pt_br, pt_bl], dtype=\"float32\")\n",
        "                dst = np.array([[0,0], [cell_size,0], [cell_size,cell_size], [0,cell_size]], dtype=\"float32\")\n",
        "                M = cv2.getPerspectiveTransform(src, dst)\n",
        "\n",
        "                # --- CHANGE 2: BORDER REPLICATE ---\n",
        "                # Smears the edge texture into empty space instead of adding black walls\n",
        "                warped = cv2.warpPerspective(img, M, (cell_size, cell_size), borderMode=cv2.BORDER_REPLICATE)\n",
        "\n",
        "                # Crop border & Place\n",
        "                warped = warped[2:-2, 2:-2]\n",
        "                warped = cv2.resize(warped, (cell_size, cell_size))\n",
        "\n",
        "                # Determine placement based on X-coord relative to image width\n",
        "                c_idx = int(pt_top[0] / (w / num_cols_est))\n",
        "                y_pos, x_pos = r * cell_size, c_idx * cell_size\n",
        "\n",
        "                if y_pos+cell_size <= canvas_h and x_pos+cell_size <= canvas_w:\n",
        "                    final_grid[y_pos:y_pos+cell_size, x_pos:x_pos+cell_size] = warped\n",
        "                    cells_warped += 1\n",
        "\n",
        "        if cells_warped < 20: return fallback_img, \"Fallback: Warp Failed\"\n",
        "        return final_grid, \"Active: Gridded\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return fallback_img, f\"Fallback: Error\"\n",
        "\n",
        "# ==========================================\n",
        "#    BATCH VISUALIZATION\n",
        "# ==========================================\n",
        "\n",
        "def visualize_batch_processing():\n",
        "    all_images = sorted(list(TEST_IMG_DIR.glob(\"*.png\")))\n",
        "    if not all_images:\n",
        "        print(\"❌ No images found in directory.\")\n",
        "        return\n",
        "\n",
        "    # Select random sample\n",
        "    sample_paths = random.sample(all_images, min(NUM_TO_VISUALIZE, len(all_images)))\n",
        "\n",
        "    results = []\n",
        "    print(f\"Processing {len(sample_paths)} images...\")\n",
        "    for p in tqdm(sample_paths):\n",
        "        orig = cv2.imread(str(p))\n",
        "        processed, status = reconstruct_grid_safe_with_status(p)\n",
        "        results.append((p.name, orig, processed, status))\n",
        "\n",
        "    # Setup Grid Plot: 10 rows, 10 columns (5 pairs per row)\n",
        "    rows = 10\n",
        "    cols_per_pair = 2\n",
        "    total_cols = 5 * cols_per_pair\n",
        "\n",
        "    fig, axes = plt.subplots(rows, total_cols, figsize=(20, 25))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    active_count = 0\n",
        "\n",
        "    for i, (name, orig, proc, status) in enumerate(results):\n",
        "        if i >= len(axes)//2: break\n",
        "\n",
        "        ax_orig = axes[i * 2]\n",
        "        ax_proc = axes[i * 2 + 1]\n",
        "\n",
        "        # Original\n",
        "        if orig is not None:\n",
        "            ax_orig.imshow(cv2.cvtColor(orig, cv2.COLOR_BGR2RGB))\n",
        "        ax_orig.set_title(f\"{name}\\nOriginal\", fontsize=9)\n",
        "        ax_orig.axis('off')\n",
        "\n",
        "        # Processed\n",
        "        ax_proc.imshow(cv2.cvtColor(proc, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        # Color-code title based on status\n",
        "        title_color = 'green' if status.startswith(\"Active\") else 'orangered'\n",
        "        if status.startswith(\"Active\"): active_count += 1\n",
        "\n",
        "        ax_proc.set_title(status, fontsize=9, color=title_color, fontweight='bold')\n",
        "        ax_proc.axis('off')\n",
        "\n",
        "    # Hide unused axes\n",
        "    for j in range(len(results)*2, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(f\"Batch Processing Report: {active_count}/{len(results)} Active Grid Reconstruction\", y=1.02, fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "# Run the report\n",
        "visualize_batch_processing()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-20T19:19:34.092499Z",
          "iopub.execute_input": "2025-12-20T19:19:34.092888Z",
          "iopub.status.idle": "2025-12-20T19:20:03.095431Z",
          "shell.execute_reply.started": "2025-12-20T19:19:34.092856Z",
          "shell.execute_reply": "2025-12-20T19:20:03.091853Z"
        },
        "id": "9oAEhzCHqMBf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import DBSCAN\n",
        "from pathlib import Path\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "TEST_IMG_DIR = Path(\"/kaggle/input/the-blind-flight-synapse-drive-ps-1/SynapseDrive_Dataset/test/images\")\n",
        "CELL_SIZE = 40\n",
        "NUM_TO_VISUALIZE = 50\n",
        "def tiny_deskew(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    edges = cv2.Canny(gray, 50, 150)\n",
        "    lines = cv2.HoughLines(edges, 1, np.pi/180, 250)\n",
        "\n",
        "    if lines is None:\n",
        "        return img\n",
        "\n",
        "    angles = []\n",
        "    for rho, theta in lines[:,0]:\n",
        "        ang = (theta - np.pi/2) * 180 / np.pi\n",
        "        if abs(ang) < 20:\n",
        "            angles.append(ang)\n",
        "\n",
        "    if not angles:\n",
        "        return img\n",
        "\n",
        "    angle = np.median(angles)\n",
        "    if abs(angle) < 3:\n",
        "        return img\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "    M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
        "    return cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REFLECT_101)\n",
        "def preprocess_for_grid(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    blur = cv2.medianBlur(gray, 7)\n",
        "    return cv2.adaptiveThreshold(\n",
        "        blur, 255,\n",
        "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "        cv2.THRESH_BINARY_INV,\n",
        "        19, 5\n",
        "    )\n",
        "def get_intersections(img):\n",
        "    thresh = preprocess_for_grid(img)\n",
        "    scale = 25\n",
        "\n",
        "    h_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (scale, 5))\n",
        "    v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, scale))\n",
        "\n",
        "    mask_h = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, h_kernel)\n",
        "    mask_v = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, v_kernel)\n",
        "\n",
        "    intersections = cv2.bitwise_and(mask_h, mask_v)\n",
        "    intersections = cv2.dilate(intersections, np.ones((5,5)))\n",
        "\n",
        "    num_labels, _, stats, centroids = cv2.connectedComponentsWithStats(intersections)\n",
        "    pts = [centroids[i] for i in range(1, num_labels)\n",
        "           if stats[i, cv2.CC_STAT_AREA] > 15]\n",
        "\n",
        "    if not pts:\n",
        "        return np.array([])\n",
        "\n",
        "    pts = np.array(pts)\n",
        "    clustering = DBSCAN(eps=20, min_samples=1).fit(pts)\n",
        "\n",
        "    return np.array([\n",
        "        pts[clustering.labels_ == k].mean(axis=0)\n",
        "        for k in set(clustering.labels_)\n",
        "    ])\n",
        "def sort_points_robust(points):\n",
        "    y_clustering = DBSCAN(eps=30, min_samples=3).fit(points[:,1:2])\n",
        "\n",
        "    rows = {}\n",
        "    for pt, lbl in zip(points, y_clustering.labels_):\n",
        "        if lbl == -1:\n",
        "            continue\n",
        "        rows.setdefault(lbl, []).append(pt)\n",
        "\n",
        "    sorted_rows = sorted(\n",
        "        rows.values(),\n",
        "        key=lambda r: np.mean([p[1] for p in r])\n",
        "    )\n",
        "\n",
        "    return [np.array(sorted(row, key=lambda p: p[0])) for row in sorted_rows]\n",
        "def reconstruct_grid_safe(img):\n",
        "    img = tiny_deskew(img)\n",
        "    fallback = cv2.resize(img, (800,800))\n",
        "\n",
        "    points = get_intersections(img)\n",
        "    if len(points) < 40:\n",
        "        return fallback, \"Fallback: Low intersections\"\n",
        "\n",
        "    rows = sort_points_robust(points)\n",
        "    if len(rows) < 8:\n",
        "        return fallback, \"Fallback: Few rows\"\n",
        "\n",
        "    num_rows = len(rows)\n",
        "    num_cols = int(np.median([len(r) for r in rows]))\n",
        "    if num_cols < 8:\n",
        "        return fallback, \"Fallback: Few cols\"\n",
        "\n",
        "    canvas = np.ones(\n",
        "        ((num_rows-1)*CELL_SIZE, (num_cols-1)*CELL_SIZE, 3),\n",
        "        dtype=np.uint8\n",
        "    ) * 128\n",
        "\n",
        "    warped = 0\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    for r in range(num_rows - 1):\n",
        "        top = rows[r]\n",
        "        bottom = rows[r+1]\n",
        "\n",
        "        for pt_tl in top:\n",
        "            rights = [p for p in top if p[0] > pt_tl[0]]\n",
        "            if not rights:\n",
        "                continue\n",
        "            pt_tr = min(rights, key=lambda p: p[0])\n",
        "\n",
        "            # 🔧 tighter skew limit\n",
        "            if abs(pt_tr[0] - pt_tl[0]) > 90:\n",
        "                continue\n",
        "\n",
        "            bls = [p for p in bottom if abs(p[0] - pt_tl[0]) < 70]\n",
        "            brs = [p for p in bottom if abs(p[0] - pt_tr[0]) < 70]\n",
        "            if not bls or not brs:\n",
        "                continue\n",
        "\n",
        "            pt_bl = min(bls, key=lambda p: abs(p[0] - pt_tl[0]))\n",
        "            pt_br = min(brs, key=lambda p: abs(p[0] - pt_tr[0]))\n",
        "\n",
        "            src = np.array([pt_tl, pt_tr, pt_br, pt_bl], dtype=np.float32)\n",
        "\n",
        "            # 🔧 quad safety\n",
        "            if cv2.contourArea(src) < 150:\n",
        "                continue\n",
        "            if not cv2.isContourConvex(src):\n",
        "                continue\n",
        "\n",
        "            dst = np.array([\n",
        "                [0,0], [CELL_SIZE,0],\n",
        "                [CELL_SIZE,CELL_SIZE], [0,CELL_SIZE]\n",
        "            ], dtype=np.float32)\n",
        "\n",
        "            M = cv2.getPerspectiveTransform(src, dst)\n",
        "            cell = cv2.warpPerspective(\n",
        "                img, M, (CELL_SIZE, CELL_SIZE),\n",
        "                borderMode=cv2.BORDER_REPLICATE\n",
        "            )\n",
        "\n",
        "            c_idx = int(pt_tl[0] / (w / num_cols))\n",
        "            y, x = r*CELL_SIZE, c_idx*CELL_SIZE\n",
        "\n",
        "            if y+CELL_SIZE <= canvas.shape[0] and x+CELL_SIZE <= canvas.shape[1]:\n",
        "                canvas[y:y+CELL_SIZE, x:x+CELL_SIZE] = cell\n",
        "                warped += 1\n",
        "\n",
        "    if warped < 20:\n",
        "        return fallback, \"Fallback: Warp failed\"\n",
        "\n",
        "    return canvas, \"Active: Gridded\"\n",
        "def visualize_batch():\n",
        "    imgs = list(TEST_IMG_DIR.glob(\"*.png\"))\n",
        "    samples = random.sample(imgs, min(NUM_TO_VISUALIZE, len(imgs)))\n",
        "\n",
        "    fig, axes = plt.subplots(len(samples), 2, figsize=(10, 3*len(samples)))\n",
        "\n",
        "    for i, p in enumerate(tqdm(samples)):\n",
        "        img = cv2.imread(str(p))\n",
        "        out, status = reconstruct_grid_safe(img)\n",
        "\n",
        "        axes[i,0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "        axes[i,0].set_title(\"Original\")\n",
        "        axes[i,0].axis(\"off\")\n",
        "\n",
        "        axes[i,1].imshow(cv2.cvtColor(out, cv2.COLOR_BGR2RGB))\n",
        "        axes[i,1].set_title(status, color=\"green\" if \"Active\" in status else \"red\")\n",
        "        axes[i,1].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "visualize_batch()\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-20T19:31:54.794855Z",
          "iopub.execute_input": "2025-12-20T19:31:54.795294Z",
          "iopub.status.idle": "2025-12-20T19:32:38.339592Z",
          "shell.execute_reply.started": "2025-12-20T19:31:54.795251Z",
          "shell.execute_reply": "2025-12-20T19:32:38.337875Z"
        },
        "id": "G6e_MtOVqMBh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import DBSCAN\n",
        "from pathlib import Path\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- CONFIG ---\n",
        "TEST_IMG_DIR = Path(\"/kaggle/input/the-blind-flight-synapse-drive-ps-1/SynapseDrive_Dataset/test/images\")\n",
        "NUM_TO_VISUALIZE = 50\n",
        "\n",
        "# ==========================================\n",
        "#    CORE GRID PROCESSING FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def preprocess_for_grid(img):\n",
        "    \"\"\" Removes texture (Lab/Forest) but keeps black lines. \"\"\"\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    # Median Blur is crucial for killing texture noise\n",
        "    no_texture = cv2.medianBlur(gray, 7)\n",
        "    thresh = cv2.adaptiveThreshold(no_texture, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                   cv2.THRESH_BINARY_INV, 19, 5)\n",
        "    return thresh\n",
        "\n",
        "def get_intersections(img):\n",
        "    thresh = preprocess_for_grid(img)\n",
        "    scale = 25\n",
        "    h_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (scale, 1))\n",
        "    v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, scale))\n",
        "    mask_h = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, h_kernel)\n",
        "    mask_v = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, v_kernel)\n",
        "    intersections = cv2.bitwise_and(mask_h, mask_v)\n",
        "    intersections = cv2.dilate(intersections, np.ones((5,5)))\n",
        "\n",
        "    num_labels, _, stats, centroids = cv2.connectedComponentsWithStats(intersections)\n",
        "    points = []\n",
        "    for i in range(1, num_labels):\n",
        "        if stats[i, cv2.CC_STAT_AREA] > 10: points.append(centroids[i])\n",
        "\n",
        "    if not points: return np.array([])\n",
        "\n",
        "    # DBSCAN Clustering to merge double-detections\n",
        "    points = np.array(points)\n",
        "    clustering = DBSCAN(eps=20, min_samples=1).fit(points)\n",
        "    clean_points = []\n",
        "    for label in set(clustering.labels_):\n",
        "        clean_points.append(np.mean(points[clustering.labels_ == label], axis=0))\n",
        "    return np.array(clean_points)\n",
        "\n",
        "def sort_points_robust(points):\n",
        "    # Cluster Y-coordinates to define Rows\n",
        "    y_clustering = DBSCAN(eps=25, min_samples=3).fit(points[:, 1].reshape(-1, 1))\n",
        "    rows_dict = {}\n",
        "    for pt, label in zip(points, y_clustering.labels_):\n",
        "        if label == -1: continue\n",
        "        if label not in rows_dict: rows_dict[label] = []\n",
        "        rows_dict[label].append(pt)\n",
        "\n",
        "    # Sort rows top-to-bottom, points left-to-right\n",
        "    sorted_keys = sorted(rows_dict.keys(), key=lambda k: np.mean([p[1] for p in rows_dict[k]]))\n",
        "    final_rows = []\n",
        "    for k in sorted_keys:\n",
        "        row_pts = sorted(rows_dict[k], key=lambda p: p[0])\n",
        "        final_rows.append(np.array(row_pts))\n",
        "    return final_rows\n",
        "\n",
        "def reconstruct_grid_safe_with_status(image_path, cell_size=40):\n",
        "    \"\"\" Returns (processed_image, status_string) \"\"\"\n",
        "    img = cv2.imread(str(image_path))\n",
        "    if img is None: return np.zeros((800,800,3), dtype=np.uint8), \"Load Error\"\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "    fallback_img = cv2.resize(img, (800, 800))\n",
        "\n",
        "    try:\n",
        "        points = get_intersections(img)\n",
        "        # VALIDATION THRESHOLDS\n",
        "        if len(points) < 40: return fallback_img, f\"Fallback: Low Pts ({len(points)})\"\n",
        "\n",
        "        grid_rows = sort_points_robust(points)\n",
        "        if len(grid_rows) < 8: return fallback_img, f\"Fallback: Few Rows ({len(grid_rows)})\"\n",
        "\n",
        "        # WARPING LOOP\n",
        "        num_rows = len(grid_rows)\n",
        "        # Use median row length to estimate grid width\n",
        "        row_lens = [len(r) for r in grid_rows]\n",
        "        if not row_lens: return fallback_img, \"Fallback: Empty Rows\"\n",
        "        num_cols_est = int(np.median(row_lens))\n",
        "        if num_cols_est < 8: return fallback_img, f\"Fallback: Narrow Grid ({num_cols_est} cols)\"\n",
        "\n",
        "        canvas_h = (num_rows - 1) * cell_size\n",
        "        canvas_w = (num_cols_est - 1) * cell_size\n",
        "        # Grey background to show gaps\n",
        "        final_grid = np.ones((canvas_h, canvas_w, 3), dtype=np.uint8) * 128\n",
        "        cells_warped = 0\n",
        "\n",
        "        for r in range(num_rows - 1):\n",
        "            row_top = grid_rows[r]\n",
        "            row_btm = grid_rows[r+1]\n",
        "\n",
        "            for pt_top in row_top:\n",
        "                # 1. Find TR neighbor\n",
        "                neighbors_tr = [p for p in row_top if p[0] > pt_top[0]]\n",
        "                if not neighbors_tr: continue\n",
        "                pt_tr = min(neighbors_tr, key=lambda p: p[0])\n",
        "                if abs(pt_tr[0] - pt_top[0]) > 100: continue # Gap too big\n",
        "\n",
        "                # 2. Find BL match in bottom row\n",
        "                candidates_bl = [p for p in row_btm if abs(p[0] - pt_top[0]) < 70]\n",
        "                if not candidates_bl: continue\n",
        "                pt_bl = min(candidates_bl, key=lambda p: abs(p[0] - pt_top[0]))\n",
        "\n",
        "                # 3. Find BR match in bottom row\n",
        "                candidates_br = [p for p in row_btm if abs(p[0] - pt_tr[0]) < 70]\n",
        "                if not candidates_br: continue\n",
        "                pt_br = min(candidates_br, key=lambda p: abs(p[0] - pt_tr[0]))\n",
        "\n",
        "                # Warp\n",
        "                src = np.array([pt_top, pt_tr, pt_br, pt_bl], dtype=\"float32\")\n",
        "                dst = np.array([[0,0], [cell_size,0], [cell_size,cell_size], [0,cell_size]], dtype=\"float32\")\n",
        "                M = cv2.getPerspectiveTransform(src, dst)\n",
        "                warped = cv2.warpPerspective(img, M, (cell_size, cell_size))\n",
        "\n",
        "                # Crop border & Place\n",
        "                warped = warped[2:-2, 2:-2]\n",
        "                warped = cv2.resize(warped, (cell_size, cell_size))\n",
        "\n",
        "                # Determine placement based on X-coord relative to image width\n",
        "                c_idx = int(pt_top[0] / (w / num_cols_est))\n",
        "                y_pos, x_pos = r * cell_size, c_idx * cell_size\n",
        "\n",
        "                if y_pos+cell_size <= canvas_h and x_pos+cell_size <= canvas_w:\n",
        "                    final_grid[y_pos:y_pos+cell_size, x_pos:x_pos+cell_size] = warped\n",
        "                    cells_warped += 1\n",
        "\n",
        "        if cells_warped < 20: return fallback_img, \"Fallback: Warp Failed\"\n",
        "        return final_grid, \"Active: Gridded\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return fallback_img, f\"Fallback: Error\"\n",
        "\n",
        "# ==========================================\n",
        "#    BATCH VISUALIZATION\n",
        "# ==========================================\n",
        "\n",
        "def visualize_batch_processing():\n",
        "    all_images = sorted(list(TEST_IMG_DIR.glob(\"*.png\")))\n",
        "    if not all_images:\n",
        "        print(\"❌ No images found in directory.\")\n",
        "        return\n",
        "\n",
        "    # Select random sample\n",
        "    sample_paths = random.sample(all_images, min(NUM_TO_VISUALIZE, len(all_images)))\n",
        "\n",
        "    results = []\n",
        "    print(f\"Processing {len(sample_paths)} images...\")\n",
        "    for p in tqdm(sample_paths):\n",
        "        orig = cv2.imread(str(p))\n",
        "        processed, status = reconstruct_grid_safe_with_status(p)\n",
        "        results.append((p.name, orig, processed, status))\n",
        "\n",
        "    # Setup Grid Plot: 10 rows, 10 columns (5 pairs per row)\n",
        "    rows = 10\n",
        "    cols_per_pair = 2\n",
        "    total_cols = 5 * cols_per_pair\n",
        "\n",
        "    fig, axes = plt.subplots(rows, total_cols, figsize=(20, 25))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    active_count = 0\n",
        "\n",
        "    for i, (name, orig, proc, status) in enumerate(results):\n",
        "        if i >= len(axes)//2: break\n",
        "\n",
        "        ax_orig = axes[i * 2]\n",
        "        ax_proc = axes[i * 2 + 1]\n",
        "\n",
        "        # Original\n",
        "        if orig is not None:\n",
        "            ax_orig.imshow(cv2.cvtColor(orig, cv2.COLOR_BGR2RGB))\n",
        "        ax_orig.set_title(f\"{name}\\nOriginal\", fontsize=9)\n",
        "        ax_orig.axis('off')\n",
        "\n",
        "        # Processed\n",
        "        ax_proc.imshow(cv2.cvtColor(proc, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        # Color-code title based on status\n",
        "        title_color = 'green' if status.startswith(\"Active\") else 'orangered'\n",
        "        if status.startswith(\"Active\"): active_count += 1\n",
        "\n",
        "        ax_proc.set_title(status, fontsize=9, color=title_color, fontweight='bold')\n",
        "        ax_proc.axis('off')\n",
        "\n",
        "    # Hide unused axes\n",
        "    for j in range(len(results)*2, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(f\"Batch Processing Report: {active_count}/{len(results)} Active Grid Reconstruction\", y=1.02, fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "# Run the report\n",
        "visualize_batch_processing()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-20T19:38:49.064659Z",
          "iopub.execute_input": "2025-12-20T19:38:49.065507Z",
          "iopub.status.idle": "2025-12-20T19:39:18.130388Z",
          "shell.execute_reply.started": "2025-12-20T19:38:49.06547Z",
          "shell.execute_reply": "2025-12-20T19:39:18.129418Z"
        },
        "id": "SHf_3SJMqMBi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import shutil\n",
        "\n",
        "# --- CONFIG ---\n",
        "TRAIN_IMG_DIR = Path(\"/kaggle/input/the-blind-flight-synapse-drive-ps-1/SynapseDrive_Dataset/train/images\")\n",
        "TRAIN_MASK_DIR = Path(\"/kaggle/input/the-blind-flight-synapse-drive-ps-1/SynapseDrive_Dataset/train/masks\")\n",
        "OUTPUT_DATASET_DIR = Path(\"./cnn_dataset\")\n",
        "\n",
        "# Classes based on Mask Colors\n",
        "# 0: Lab/Road (RGB: 128,128,128 or similar) -> We'll define ranges\n",
        "# 1: Wall (Black)\n",
        "# 2: Mud/Forest/Sand (Texture)\n",
        "# 3: Start (Green)\n",
        "# 4: Goal (Red)\n",
        "\n",
        "# Setup Directories\n",
        "for category in [\"road\", \"wall\", \"mud\", \"start\", \"goal\"]:\n",
        "    os.makedirs(OUTPUT_DATASET_DIR / category, exist_ok=True)\n",
        "\n",
        "# Reuse your BEST Slicer Function\n",
        "def get_grid_cells_and_masks(img, mask, cell_size=40):\n",
        "    \"\"\"\n",
        "    Slices both the image and the mask using the robust grid logic.\n",
        "    Returns a list of (cell_img, cell_mask_patch).\n",
        "    \"\"\"\n",
        "    # ... (Insert your 'preprocess_for_grid' and 'get_intersections' logic here) ...\n",
        "    # For brevity, I will use a simplified \"Resize & Slice\" logic which mimics\n",
        "    # what we will do at inference time for stability.\n",
        "\n",
        "    # 1. Resize both to 800x800 (Force Grid Alignment)\n",
        "    img_r = cv2.resize(img, (800, 800))\n",
        "    mask_r = cv2.resize(mask, (800, 800), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    cells = []\n",
        "\n",
        "    # 2. Slice strictly into 20x20\n",
        "    for r in range(20):\n",
        "        for c in range(20):\n",
        "            y1, y2 = r*cell_size, (r+1)*cell_size\n",
        "            x1, x2 = c*cell_size, (c+1)*cell_size\n",
        "\n",
        "            img_cell = img_r[y1:y2, x1:x2]\n",
        "            mask_cell = mask_r[y1:y2, x1:x2]\n",
        "\n",
        "            # Optional: Crop border to remove grid lines\n",
        "            crop = 3\n",
        "            img_cell = img_cell[crop:-crop, crop:-crop]\n",
        "            mask_cell = mask_cell[crop:-crop, crop:-crop]\n",
        "\n",
        "            cells.append((img_cell, mask_cell))\n",
        "\n",
        "    return cells\n",
        "\n",
        "def identify_label(mask_cell):\n",
        "    \"\"\"\n",
        "    Determines the class of the cell based on the ground truth mask.\n",
        "    \"\"\"\n",
        "    # Count unique colors\n",
        "    # We look at the center pixel or majority vote\n",
        "\n",
        "    # Simple Logic: Check presence of specific colors\n",
        "    # Mask format is likely (H, W, 3) BGR\n",
        "\n",
        "    # 1. Check for Goal (Red) - BGR: (0, 0, 255) approx\n",
        "    red_mask = cv2.inRange(mask_cell, (0, 0, 200), (50, 50, 255))\n",
        "    if np.sum(red_mask) > 10: return \"goal\"\n",
        "\n",
        "    # 2. Check for Start (Green) - BGR: (0, 200, 0) approx\n",
        "    green_mask = cv2.inRange(mask_cell, (0, 200, 0), (50, 255, 50))\n",
        "    if np.sum(green_mask) > 10: return \"start\"\n",
        "\n",
        "    # 3. Check for Wall (Black)\n",
        "    # Walls are strictly black (0,0,0)\n",
        "    black_pixels = np.sum(np.all(mask_cell < 30, axis=-1))\n",
        "    total_pixels = mask_cell.shape[0] * mask_cell.shape[1]\n",
        "    if black_pixels > total_pixels * 0.5: return \"wall\"\n",
        "\n",
        "    # 4. Check for Mud/Texture\n",
        "    # This depends on the specific mask color coding for mud.\n",
        "    # Usually mud is Brown/Orange or similar.\n",
        "    # If not Wall, Start, Goal, or Road (Grey), it's Mud.\n",
        "\n",
        "    # Road/Lab floor is usually greyish or white in masks?\n",
        "    # Let's assume anything else is Mud for now, or check training mask palette.\n",
        "    # For this snippet, let's assume Road is the default if not others.\n",
        "\n",
        "    return \"road\" # Default\n",
        "\n",
        "# --- EXECUTION ---\n",
        "img_paths = sorted(list(TRAIN_IMG_DIR.glob(\"*.png\")))\n",
        "mask_paths = sorted(list(TRAIN_MASK_DIR.glob(\"*.png\")))\n",
        "\n",
        "print(f\"Generating dataset from {len(img_paths)} images...\")\n",
        "\n",
        "count = 0\n",
        "for img_p, mask_p in tqdm(zip(img_paths, mask_paths), total=len(img_paths)):\n",
        "    img = cv2.imread(str(img_p))\n",
        "    mask = cv2.imread(str(mask_p))\n",
        "\n",
        "    # Use the robust slicer (or the simple resizer for training data generation)\n",
        "    # Using Simple Resizer ensures we get perfect alignment for training labels\n",
        "    cell_data = get_grid_cells_and_masks(img, mask)\n",
        "\n",
        "    for i, (c_img, c_mask) in enumerate(cell_data):\n",
        "        label = identify_label(c_mask)\n",
        "\n",
        "        # Save\n",
        "        filename = f\"{img_p.stem}_cell_{i}.png\"\n",
        "        save_path = OUTPUT_DATASET_DIR / label / filename\n",
        "\n",
        "        # Only save valid chunks\n",
        "        if c_img.shape[0] > 10 and c_img.shape[1] > 10:\n",
        "            cv2.imwrite(str(save_path), c_img)\n",
        "            count += 1\n",
        "\n",
        "print(f\"✅ Generated {count} labeled cell images!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-20T20:05:36.935902Z",
          "iopub.execute_input": "2025-12-20T20:05:36.936343Z",
          "iopub.status.idle": "2025-12-20T20:05:36.984768Z",
          "shell.execute_reply.started": "2025-12-20T20:05:36.936308Z",
          "shell.execute_reply": "2025-12-20T20:05:36.983521Z"
        },
        "id": "at5kdbIwqMBk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import DBSCAN\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- CONFIG ---\n",
        "TEST_IMG_DIR = Path(\"/kaggle/input/the-blind-flight-synapse-drive-ps-1/SynapseDrive_Dataset/test/images\")\n",
        "CELL_SIZE = 64         # Fixed size for CNN input\n",
        "MAX_ROWS = 20\n",
        "MAX_COLS = 20\n",
        "\n",
        "# ==========================================\n",
        "#  CORE HELPERS (Unchanged)\n",
        "# ==========================================\n",
        "def preprocess_for_grid(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    no_texture = cv2.medianBlur(gray, 7)\n",
        "    thresh = cv2.adaptiveThreshold(no_texture, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                   cv2.THRESH_BINARY_INV, 19, 5)\n",
        "    return thresh\n",
        "\n",
        "def get_intersections(img):\n",
        "    thresh = preprocess_for_grid(img)\n",
        "    scale = 25\n",
        "    h_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (scale, 1))\n",
        "    v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, scale))\n",
        "    mask_h = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, h_kernel)\n",
        "    mask_v = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, v_kernel)\n",
        "    intersections = cv2.bitwise_and(mask_h, mask_v)\n",
        "    intersections = cv2.dilate(intersections, np.ones((5,5)))\n",
        "\n",
        "    num_labels, _, stats, centroids = cv2.connectedComponentsWithStats(intersections)\n",
        "    points = []\n",
        "    for i in range(1, num_labels):\n",
        "        if stats[i, cv2.CC_STAT_AREA] > 10: points.append(centroids[i])\n",
        "\n",
        "    if not points: return np.array([])\n",
        "\n",
        "    points = np.array(points)\n",
        "    clustering = DBSCAN(eps=20, min_samples=1).fit(points)\n",
        "    clean_points = []\n",
        "    for label in set(clustering.labels_):\n",
        "        clean_points.append(np.mean(points[clustering.labels_ == label], axis=0))\n",
        "    return np.array(clean_points)\n",
        "\n",
        "def sort_points_robust(points):\n",
        "    y_clustering = DBSCAN(eps=25, min_samples=3).fit(points[:, 1].reshape(-1, 1))\n",
        "    rows_dict = {}\n",
        "    for pt, label in zip(points, y_clustering.labels_):\n",
        "        if label == -1: continue\n",
        "        if label not in rows_dict: rows_dict[label] = []\n",
        "        rows_dict[label].append(pt)\n",
        "\n",
        "    # Sort rows by Y, then points in row by X\n",
        "    sorted_keys = sorted(rows_dict.keys(), key=lambda k: np.mean([p[1] for p in rows_dict[k]]))\n",
        "    final_rows = []\n",
        "    for k in sorted_keys:\n",
        "        row_pts = sorted(rows_dict[k], key=lambda p: p[0])\n",
        "        final_rows.append(np.array(row_pts))\n",
        "    return final_rows\n",
        "\n",
        "# ==========================================\n",
        "#  NEW: GLOBAL COLUMN ALIGNMENT\n",
        "# ==========================================\n",
        "def get_global_column_grid(grid_rows):\n",
        "    \"\"\"\n",
        "    Collects all X-coords and clusters them to find the canonical\n",
        "    vertical grid lines valid for the ENTIRE image.\n",
        "    \"\"\"\n",
        "    all_x = []\n",
        "    for row in grid_rows:\n",
        "        for pt in row:\n",
        "            all_x.append(pt[0])\n",
        "\n",
        "    if not all_x: return np.array([])\n",
        "\n",
        "    all_x = np.array(all_x).reshape(-1, 1)\n",
        "\n",
        "    # Cluster X-coordinates (Vertical Lines)\n",
        "    # eps=15 means points within 15px horizontally are considered the same column line\n",
        "    clustering = DBSCAN(eps=15, min_samples=1).fit(all_x)\n",
        "\n",
        "    unique_labels = set(clustering.labels_)\n",
        "    col_centers = []\n",
        "    for label in unique_labels:\n",
        "        if label == -1: continue\n",
        "        # Average x for this vertical line\n",
        "        center = np.mean(all_x[clustering.labels_ == label])\n",
        "        col_centers.append(center)\n",
        "\n",
        "    # Sort left to right\n",
        "    return np.array(sorted(col_centers))\n",
        "\n",
        "# ==========================================\n",
        "#  UPDATED EXTRACTOR\n",
        "# ==========================================\n",
        "def extract_grid_data(image_path):\n",
        "    img = cv2.imread(str(image_path))\n",
        "    if img is None: return None, None, \"Load Error\"\n",
        "\n",
        "    grid_tensor = np.zeros((MAX_ROWS, MAX_COLS, CELL_SIZE, CELL_SIZE, 3), dtype=np.uint8)\n",
        "    mask_tensor = np.zeros((MAX_ROWS, MAX_COLS), dtype=np.uint8)\n",
        "\n",
        "    try:\n",
        "        points = get_intersections(img)\n",
        "        if len(points) < 40: return grid_tensor, mask_tensor, \"Fallback: Low Pts\"\n",
        "\n",
        "        grid_rows = sort_points_robust(points)\n",
        "        if len(grid_rows) < 4: return grid_tensor, mask_tensor, \"Fallback: Few Rows\"\n",
        "\n",
        "        # --- FIX STEP: Get Canonical Columns ---\n",
        "        col_centers = get_global_column_grid(grid_rows)\n",
        "        if len(col_centers) < 2: return grid_tensor, mask_tensor, \"Fallback: No Cols\"\n",
        "\n",
        "        # We can also verify row alignment if needed, but rows are usually cleaner\n",
        "\n",
        "        for r in range(min(len(grid_rows) - 1, MAX_ROWS)):\n",
        "            row_top = grid_rows[r]\n",
        "            row_btm = grid_rows[r+1]\n",
        "\n",
        "            for pt_top in row_top:\n",
        "                # 1. Find neighbors (Same as before)\n",
        "                neighbors_tr = [p for p in row_top if p[0] > pt_top[0]]\n",
        "                if not neighbors_tr: continue\n",
        "                pt_tr = min(neighbors_tr, key=lambda p: p[0])\n",
        "                if abs(pt_tr[0] - pt_top[0]) > 100: continue\n",
        "\n",
        "                candidates_bl = [p for p in row_btm if abs(p[0] - pt_top[0]) < 70]\n",
        "                if not candidates_bl: continue\n",
        "                pt_bl = min(candidates_bl, key=lambda p: abs(p[0] - pt_top[0]))\n",
        "\n",
        "                candidates_br = [p for p in row_btm if abs(p[0] - pt_tr[0]) < 70]\n",
        "                if not candidates_br: continue\n",
        "                pt_br = min(candidates_br, key=lambda p: abs(p[0] - pt_tr[0]))\n",
        "\n",
        "                # Warp\n",
        "                src = np.array([pt_top, pt_tr, pt_br, pt_bl], dtype=\"float32\")\n",
        "                dst = np.array([[0,0], [CELL_SIZE,0], [CELL_SIZE,CELL_SIZE], [0,CELL_SIZE]], dtype=\"float32\")\n",
        "                M = cv2.getPerspectiveTransform(src, dst)\n",
        "                warped = cv2.warpPerspective(img, M, (CELL_SIZE, CELL_SIZE))\n",
        "\n",
        "                # --- FIX STEP: Robust Indexing ---\n",
        "                # Find which canonical column this cell starts at\n",
        "                # We compare pt_top[0] (left edge) to our known column lines\n",
        "                diffs = np.abs(col_centers - pt_top[0])\n",
        "                c_idx = np.argmin(diffs)\n",
        "\n",
        "                if c_idx < MAX_COLS:\n",
        "                    grid_tensor[r, c_idx] = warped\n",
        "                    mask_tensor[r, c_idx] = 1\n",
        "\n",
        "        if np.sum(mask_tensor) < 20: return grid_tensor, mask_tensor, \"Fallback: Warp Failed\"\n",
        "        return grid_tensor, mask_tensor, \"Active: Gridded\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return grid_tensor, mask_tensor, f\"Error: {str(e)}\"\n",
        "\n",
        "# ==========================================\n",
        "#  VISUALIZATION\n",
        "# ==========================================\n",
        "# 1. Pick an image\n",
        "img_path = list(TEST_IMG_DIR.glob(\"*.png\"))[0]\n",
        "\n",
        "# 2. Extract Data\n",
        "grid_data, valid_mask, status = extract_grid_data(img_path)\n",
        "\n",
        "print(f\"Status: {status}\")\n",
        "print(f\"Grid Shape: {grid_data.shape}\")\n",
        "print(f\"Valid Cells Found: {np.sum(valid_mask)}\")\n",
        "\n",
        "# 3. Reconstruct\n",
        "reconstruction = np.zeros((MAX_ROWS * CELL_SIZE, MAX_COLS * CELL_SIZE, 3), dtype=np.uint8)\n",
        "\n",
        "for r in range(MAX_ROWS):\n",
        "    for c in range(MAX_COLS):\n",
        "        if valid_mask[r, c] == 1:\n",
        "            reconstruction[r*CELL_SIZE:(r+1)*CELL_SIZE, c*CELL_SIZE:(c+1)*CELL_SIZE] = grid_data[r, c]\n",
        "        else:\n",
        "            # Mark empty/grey spots clearly\n",
        "            cv2.rectangle(reconstruction,\n",
        "                         (c*CELL_SIZE, r*CELL_SIZE),\n",
        "                         ((c+1)*CELL_SIZE, (r+1)*CELL_SIZE),\n",
        "                         (30, 30, 30), -1)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(cv2.cvtColor(reconstruction, cv2.COLOR_BGR2RGB))\n",
        "plt.title(f\"Aligned Reconstruction: {img_path.name}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-21T05:32:30.212633Z",
          "iopub.execute_input": "2025-12-21T05:32:30.213044Z",
          "iopub.status.idle": "2025-12-21T05:32:31.146841Z",
          "shell.execute_reply.started": "2025-12-21T05:32:30.213012Z",
          "shell.execute_reply": "2025-12-21T05:32:31.145435Z"
        },
        "id": "T-9vhKOKqMBl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION\n",
        "# ==========================================\n",
        "ASSETS_ROOT = Path(\"/kaggle/input/the-blind-flight-synapse-drive-ps-1/SynapseDrive_Dataset/assets\")\n",
        "OUTPUT_DIR = Path(\"./generated_dataset_final_v7\")\n",
        "IMG_SIZE = 64\n",
        "\n",
        "BASE_SAMPLES = 600\n",
        "BOOST_SAMPLES = 1200\n",
        "MEGA_BOOST_SAMPLES = 2000 # For the problematic Desert Start/End\n",
        "\n",
        "# ==========================================\n",
        "# 2. MAPPING\n",
        "# ==========================================\n",
        "ASSET_MAP = {\n",
        "    # --- DESERT ---\n",
        "    \"t1_sand\": \"Desert_Road\",\n",
        "    \"t1_cacti\": \"Desert_Cacti\",\n",
        "    \"t1_rocks\": \"Desert_Rocks\",\n",
        "    \"t1_quicksand\": \"Desert_Hazard\",\n",
        "    \"t1_rover\": \"Desert_Start\",    # <--- MEGA TARGET\n",
        "    \"t1_goal\": \"Desert_End\",       # <--- MEGA TARGET\n",
        "\n",
        "    # --- FOREST ---\n",
        "    \"t0_dirt\": \"Forest_Road\",\n",
        "    \"t0_tree\": \"Forest_Tree\",\n",
        "    \"t0_puddle\": \"Forest_Hazard\",\n",
        "    \"t0_startship\": \"Forest_Start\",\n",
        "    \"t0_goal\": \"Forest_End\",\n",
        "\n",
        "    # --- LAB ---\n",
        "    \"t2_floor\": \"Lab_Road\",\n",
        "    \"t2_wall\": \"Lab_Wall\",\n",
        "    \"t2_plasma\": \"Lab_Plasma\",\n",
        "    \"t2_glue\": \"Lab_Hazard\",\n",
        "    \"t2_drone\": \"Lab_Start\",\n",
        "    \"t2_goal\": \"Lab_End\",\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# 3. AUGMENTATION ENGINES\n",
        "# ==========================================\n",
        "def apply_realistic_tint(img):\n",
        "    \"\"\" Subtle Blue/Grey/Green tints only \"\"\"\n",
        "    if random.random() > 0.6: return img\n",
        "\n",
        "    overlay = np.zeros_like(img)\n",
        "    mode = random.choice([\"Blue\", \"Green\", \"Grey\"])\n",
        "    b, g, r = 0, 0, 0\n",
        "    intensity = random.randint(20, 50)\n",
        "\n",
        "    if mode == \"Blue\": b = intensity + 20; g = intensity // 2\n",
        "    elif mode == \"Green\": g = intensity + 20; b = intensity // 2\n",
        "    elif mode == \"Grey\": b = intensity; g = intensity; r = intensity\n",
        "\n",
        "    overlay[:] = (b, g, r)\n",
        "    return cv2.addWeighted(img, 0.85, overlay, 0.15, 0)\n",
        "\n",
        "def augment_standard(img):\n",
        "    \"\"\" Normal augmentation for roads/walls \"\"\"\n",
        "    aug_img = img.copy()\n",
        "    if random.random() > 0.40:\n",
        "        k = random.choice([1, 2, 3])\n",
        "        aug_img = np.rot90(aug_img, k)\n",
        "    if random.random() > 0.5: aug_img = cv2.flip(aug_img, 1)\n",
        "    aug_img = apply_realistic_tint(aug_img)\n",
        "    return aug_img\n",
        "\n",
        "def augment_gentle(img):\n",
        "    \"\"\"\n",
        "    SPECIAL AUGMENTATION for Start/End.\n",
        "    - NO FLIPPING (Keeps orientation).\n",
        "    - VERY LIGHT ROTATION (Only 90 deg steps, no weird skews).\n",
        "    - MINIMAL TINT (Keep colors true).\n",
        "    \"\"\"\n",
        "    aug_img = img.copy()\n",
        "\n",
        "    # Rotation is okay, but let's keep it simple\n",
        "    if random.random() > 0.30:\n",
        "        k = random.choice([1, 2, 3])\n",
        "        aug_img = np.rot90(aug_img, k)\n",
        "\n",
        "    # NO FLIPPING! The rover/flag shape might be asymmetric.\n",
        "\n",
        "    # 80% Clean, 20% very light tint\n",
        "    if random.random() > 0.8:\n",
        "        aug_img = apply_realistic_tint(aug_img)\n",
        "\n",
        "    return aug_img\n",
        "\n",
        "# ==========================================\n",
        "# 4. GENERATOR LOOP\n",
        "# ==========================================\n",
        "def generate_final_dataset():\n",
        "    if OUTPUT_DIR.exists():\n",
        "        import shutil\n",
        "        shutil.rmtree(OUTPUT_DIR)\n",
        "    OUTPUT_DIR.mkdir(parents=True)\n",
        "\n",
        "    all_assets = list(ASSETS_ROOT.rglob(\"*.png\"))\n",
        "    total_images = 0\n",
        "\n",
        "    print(f\"Generating V7 Precision Dataset...\")\n",
        "\n",
        "    for asset_path in all_assets:\n",
        "        filename = asset_path.name.lower().replace(\".png\", \"\")\n",
        "\n",
        "        class_label = None\n",
        "        for key, label in ASSET_MAP.items():\n",
        "            if key in filename:\n",
        "                class_label = label\n",
        "                break\n",
        "\n",
        "        if class_label is None:\n",
        "            if \"rover\" in filename: class_label = \"Desert_Start\"\n",
        "            elif \"startship\" in filename: class_label = \"Forest_Start\"\n",
        "            elif \"drone\" in filename: class_label = \"Lab_Start\"\n",
        "            elif \"goal\" in filename:\n",
        "                if \"t0\" in filename: class_label = \"Forest_End\"\n",
        "                elif \"t1\" in filename: class_label = \"Desert_End\"\n",
        "                elif \"t2\" in filename: class_label = \"Lab_End\"\n",
        "                else: class_label = \"Desert_End\"\n",
        "            else: continue\n",
        "\n",
        "        class_dir = OUTPUT_DIR / class_label\n",
        "        class_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # LOGIC: HOW MANY SAMPLES?\n",
        "        count = BASE_SAMPLES\n",
        "\n",
        "        # 1. Mega Boost for Desert Start/End\n",
        "        if class_label in [\"Desert_Start\", \"Desert_End\"]:\n",
        "            count = MEGA_BOOST_SAMPLES\n",
        "\n",
        "        # 2. Regular Boost for other tricky classes\n",
        "        elif class_label in [\"Lab_Start\", \"Desert_Road\", \"Forest_Road\", \"Lab_Road\"]:\n",
        "            count = BOOST_SAMPLES\n",
        "\n",
        "        original = cv2.imread(str(asset_path))\n",
        "        if original is None: continue\n",
        "        original = cv2.resize(original, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "        for i in range(count):\n",
        "            # LOGIC: WHICH AUGMENTATION?\n",
        "            if class_label in [\"Desert_Start\", \"Desert_End\"]:\n",
        "                img = augment_gentle(original)\n",
        "            else:\n",
        "                img = augment_standard(original)\n",
        "\n",
        "            save_name = f\"{class_label}_{i}.png\"\n",
        "            cv2.imwrite(str(class_dir / save_name), img)\n",
        "            total_images += 1\n",
        "\n",
        "    print(f\"✅ Generated {total_images} V7 images.\")\n",
        "    print(f\"Classes: {sorted([d.name for d in OUTPUT_DIR.iterdir()])}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_final_dataset()\n",
        "    visualize_clean_samples()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-21T07:06:14.833906Z",
          "iopub.execute_input": "2025-12-21T07:06:14.836023Z",
          "iopub.status.idle": "2025-12-21T07:06:25.741496Z",
          "shell.execute_reply.started": "2025-12-21T07:06:14.835974Z",
          "shell.execute_reply": "2025-12-21T07:06:25.740168Z"
        },
        "id": "joOagTS3qMBm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import json\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Config\n",
        "DATA_DIR = Path(\"/kaggle/working/generated_dataset_final_v7\")\n",
        "MODEL_SAVE_PATH = \"terrain_classifier_granular7.pth\"\n",
        "MAPPING_SAVE_PATH = \"class_mapping.json\"\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 8\n",
        "LEARNING_RATE = 0.001\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load Data\n",
        "transform = transforms.ToTensor()\n",
        "full_dataset = datasets.ImageFolder(root=DATA_DIR, transform=transform)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Model\n",
        "class SimpleTerrainCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleTerrainCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleTerrainCNN(num_classes=len(full_dataset.classes)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(\"Starting Training V5 (Pink/Cyan Tints)...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Epoch {epoch+1}: Acc = {100 * correct / total:.2f}%\")\n",
        "\n",
        "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "idx_to_class = {v: k for k, v in full_dataset.class_to_idx.items()}\n",
        "with open(MAPPING_SAVE_PATH, 'w') as f:\n",
        "    json.dump(idx_to_class, f)\n",
        "print(\"✅ Saved Model V5!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-21T07:06:57.65396Z",
          "iopub.execute_input": "2025-12-21T07:06:57.654331Z",
          "iopub.status.idle": "2025-12-21T07:11:18.840805Z",
          "shell.execute_reply.started": "2025-12-21T07:06:57.654275Z",
          "shell.execute_reply": "2025-12-21T07:11:18.839844Z"
        },
        "id": "mOcrWeFSqMBn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, models\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import heapq\n",
        "import json\n",
        "from pathlib import Path\n",
        "from sklearn.cluster import DBSCAN\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION\n",
        "# ==========================================\n",
        "MODEL_PATH = \"terrain_classifier_resnet.pth\"\n",
        "MAPPING_PATH = \"class_mapping.json\"\n",
        "TEST_IMG_DIR = Path(\"/kaggle/input/the-blind-flight-synapse-drive-ps-1/SynapseDrive_Dataset/test/images\")\n",
        "CELL_SIZE = 64\n",
        "MAX_ROWS = 20\n",
        "MAX_COLS = 20\n",
        "NUM_SAMPLES = 50\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- SPLIT COST TABLES (Matches V7 Dataset) ---\n",
        "COST_TABLES = {\n",
        "    \"Desert\": {\n",
        "        \"Road\": 1.2, \"Start\": 1.2, \"End\": 2.2,\n",
        "        \"Hazard\": 3.7,\n",
        "        \"Cacti\": 999.0,\n",
        "        \"Rocks\": 999.0,\n",
        "        \"Obstacle\": 999.0,\n",
        "        \"Unknown\": 8.0\n",
        "    },\n",
        "    \"Forest\": {\n",
        "        \"Road\": 1.5, \"Start\": 1.5, \"End\": 2.5,\n",
        "        \"Hazard\": 2.8,\n",
        "        \"Tree\": 999.0,\n",
        "        \"Obstacle\": 999.0,\n",
        "        \"Unknown\": 8.0\n",
        "    },\n",
        "    \"Lab\": {\n",
        "        \"Road\": 1.0, \"Start\": 1.0, \"End\": 2.0,\n",
        "        \"Hazard\": 5.0,    # Glue\n",
        "        \"Wall\": 999.0,\n",
        "        \"Plasma\": 999.0,\n",
        "        \"Obstacle\": 999.0,\n",
        "        \"Unknown\": 8.0\n",
        "    }\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# 2. VISION HELPERS (Global Alignment)\n",
        "# ==========================================\n",
        "def preprocess_for_grid(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    no_texture = cv2.medianBlur(gray, 7)\n",
        "    thresh = cv2.adaptiveThreshold(no_texture, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                   cv2.THRESH_BINARY_INV, 19, 5)\n",
        "    return thresh\n",
        "\n",
        "def get_intersections(img):\n",
        "    thresh = preprocess_for_grid(img)\n",
        "    scale = 25\n",
        "    h_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (scale, 1))\n",
        "    v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, scale))\n",
        "    mask_h = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, h_kernel)\n",
        "    mask_v = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, v_kernel)\n",
        "    intersections = cv2.bitwise_and(mask_h, mask_v)\n",
        "    intersections = cv2.dilate(intersections, np.ones((5,5)))\n",
        "\n",
        "    num_labels, _, stats, centroids = cv2.connectedComponentsWithStats(intersections)\n",
        "    points = [centroids[i] for i in range(1, num_labels) if stats[i, cv2.CC_STAT_AREA] > 10]\n",
        "    if not points: return np.array([])\n",
        "    clustering = DBSCAN(eps=20, min_samples=1).fit(points)\n",
        "    points = np.array(points)\n",
        "    clean_points = [np.mean(points[clustering.labels_ == label], axis=0) for label in set(clustering.labels_)]\n",
        "    return np.array(clean_points)\n",
        "\n",
        "def sort_points_robust(points):\n",
        "    y_clustering = DBSCAN(eps=25, min_samples=3).fit(points[:, 1].reshape(-1, 1))\n",
        "    rows_dict = {}\n",
        "    for pt, label in zip(points, y_clustering.labels_):\n",
        "        if label == -1: continue\n",
        "        if label not in rows_dict: rows_dict[label] = []\n",
        "        rows_dict[label].append(pt)\n",
        "    sorted_keys = sorted(rows_dict.keys(), key=lambda k: np.mean([p[1] for p in rows_dict[k]]))\n",
        "    return [np.array(sorted(rows_dict[k], key=lambda p: p[0])) for k in sorted_keys]\n",
        "\n",
        "def get_global_column_grid(grid_rows):\n",
        "    all_x = [pt[0] for row in grid_rows for pt in row]\n",
        "    if not all_x: return np.array([])\n",
        "    all_x = np.array(all_x).reshape(-1, 1)\n",
        "    clustering = DBSCAN(eps=15, min_samples=1).fit(all_x)\n",
        "    col_centers = [np.mean(all_x[clustering.labels_ == label]) for label in set(clustering.labels_) if label != -1]\n",
        "    return np.array(sorted(col_centers))\n",
        "\n",
        "def extract_grid_data(image_path):\n",
        "    img = cv2.imread(str(image_path))\n",
        "    if img is None: return None, None, \"Load Error\"\n",
        "\n",
        "    grid_tensor = np.zeros((MAX_ROWS, MAX_COLS, CELL_SIZE, CELL_SIZE, 3), dtype=np.uint8)\n",
        "    mask_tensor = np.zeros((MAX_ROWS, MAX_COLS), dtype=np.uint8)\n",
        "\n",
        "    try:\n",
        "        points = get_intersections(img)\n",
        "        if len(points) < 40: return grid_tensor, mask_tensor, \"Fallback: Low Pts\"\n",
        "        grid_rows = sort_points_robust(points)\n",
        "        if len(grid_rows) < 4: return grid_tensor, mask_tensor, \"Fallback: Few Rows\"\n",
        "        col_centers = get_global_column_grid(grid_rows)\n",
        "        if len(col_centers) < 2: return grid_tensor, mask_tensor, \"Fallback: No Cols\"\n",
        "\n",
        "        for r in range(min(len(grid_rows) - 1, MAX_ROWS)):\n",
        "            row_top = grid_rows[r]\n",
        "            row_btm = grid_rows[r+1]\n",
        "            for pt_top in row_top:\n",
        "                neighbors_tr = [p for p in row_top if p[0] > pt_top[0]]\n",
        "                if not neighbors_tr: continue\n",
        "                pt_tr = min(neighbors_tr, key=lambda p: p[0])\n",
        "                if abs(pt_tr[0] - pt_top[0]) > 100: continue\n",
        "\n",
        "                candidates_bl = [p for p in row_btm if abs(p[0] - pt_top[0]) < 70]\n",
        "                if not candidates_bl: continue\n",
        "                pt_bl = min(candidates_bl, key=lambda p: abs(p[0] - pt_top[0]))\n",
        "                candidates_br = [p for p in row_btm if abs(p[0] - pt_tr[0]) < 70]\n",
        "                if not candidates_br: continue\n",
        "                pt_br = min(candidates_br, key=lambda p: abs(p[0] - pt_tr[0]))\n",
        "\n",
        "                src = np.array([pt_top, pt_tr, pt_br, pt_bl], dtype=\"float32\")\n",
        "                dst = np.array([[0,0], [CELL_SIZE,0], [CELL_SIZE,CELL_SIZE], [0,CELL_SIZE]], dtype=\"float32\")\n",
        "                M = cv2.getPerspectiveTransform(src, dst)\n",
        "                warped = cv2.warpPerspective(img, M, (CELL_SIZE, CELL_SIZE))\n",
        "\n",
        "                diffs = np.abs(col_centers - pt_top[0])\n",
        "                c_idx = np.argmin(diffs)\n",
        "                if c_idx < MAX_COLS:\n",
        "                    grid_tensor[r, c_idx] = warped\n",
        "                    mask_tensor[r, c_idx] = 1\n",
        "        return grid_tensor, mask_tensor, \"Active: Gridded\"\n",
        "    except: return grid_tensor, mask_tensor, \"Error\"\n",
        "\n",
        "# ==========================================\n",
        "# 3. RESNET MODEL LOADING\n",
        "# ==========================================\n",
        "# 1. Load Mapping First\n",
        "with open(MAPPING_PATH, 'r') as f:\n",
        "    idx_to_class = json.load(f)\n",
        "    idx_to_class = {int(k): v for k, v in idx_to_class.items()}\n",
        "\n",
        "# 2. Define & Load ResNet\n",
        "print(\"🚀 Loading ResNet-34...\")\n",
        "model = models.resnet34(pretrained=False) # We load our own weights\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(idx_to_class))\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 3. Normalization (CRITICAL for ResNet)\n",
        "transform_pipe = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\"✅ Model Loaded Successfully\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. BATCH PROCESSOR\n",
        "# ==========================================\n",
        "def process_single_image(image_path):\n",
        "    # 1. Slice\n",
        "    grid_tensor, mask_tensor, status = extract_grid_data(image_path)\n",
        "    if \"Active\" not in status: return None, \"Slicer Fail\"\n",
        "\n",
        "    rows, cols = mask_tensor.shape\n",
        "    batch_tensors = []\n",
        "    coords = []\n",
        "\n",
        "    # 2. Prepare Batch (BGR -> RGB -> Normalize)\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            if mask_tensor[r, c] == 1:\n",
        "                rgb_cell = cv2.cvtColor(grid_tensor[r, c], cv2.COLOR_BGR2RGB)\n",
        "                batch_tensors.append(transform_pipe(rgb_cell))\n",
        "                coords.append((r, c))\n",
        "\n",
        "    if not batch_tensors: return None, \"Empty Grid\"\n",
        "\n",
        "    # 3. Predict\n",
        "    batch_stack = torch.stack(batch_tensors).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(batch_stack)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "    # 4. Build Map & Biome Voting\n",
        "    terrain_map = np.full((rows, cols), \"Unknown\", dtype=object)\n",
        "    counts = {\"Desert\": 0, \"Forest\": 0, \"Lab\": 0}\n",
        "\n",
        "    for i, (r, c) in enumerate(coords):\n",
        "        class_name = idx_to_class[preds[i].item()]\n",
        "        terrain_map[r, c] = class_name\n",
        "\n",
        "        biome = class_name.split(\"_\")[0] if \"_\" in class_name else \"Lab\"\n",
        "\n",
        "        # Weight unique items higher to prevent ambiguity\n",
        "        weight = 1\n",
        "        if any(x in class_name for x in [\"Cacti\", \"Tree\", \"Plasma\", \"Wall\", \"Rocks\"]): weight = 3\n",
        "        if \"Start\" in class_name or \"End\" in class_name: weight = 2\n",
        "\n",
        "        counts[biome] = counts.get(biome, 0) + weight\n",
        "\n",
        "    dominant_biome = max(counts, key=counts.get) if counts else \"Lab\"\n",
        "    costs = COST_TABLES.get(dominant_biome, COST_TABLES[\"Lab\"])\n",
        "\n",
        "    # 5. Solver (A*)\n",
        "    start_pos, end_pos = None, None\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            if \"Start\" in terrain_map[r, c]: start_pos = (r, c)\n",
        "            if \"End\" in terrain_map[r, c]: end_pos = (r, c)\n",
        "\n",
        "    path = []\n",
        "    status_msg = \"No Path\"\n",
        "\n",
        "    if start_pos and end_pos:\n",
        "        pq = [(0, start_pos)]\n",
        "        cost_so_far = {start_pos: 0}\n",
        "        came_from = {}\n",
        "\n",
        "        while pq:\n",
        "            curr_cost, curr = heapq.heappop(pq)\n",
        "            if curr == end_pos:\n",
        "                status_msg = \"Solved\"\n",
        "                break\n",
        "\n",
        "            r, c = curr\n",
        "            for nr, nc in [(r-1,c), (r+1,c), (r,c-1), (r,c+1)]:\n",
        "                if 0 <= nr < rows and 0 <= nc < cols:\n",
        "                    label = terrain_map[nr, nc]\n",
        "\n",
        "                    if label == \"Unknown\": cell_type = \"Unknown\"\n",
        "                    else: cell_type = label.split(\"_\")[1] if \"_\" in label else label\n",
        "\n",
        "                    step_cost = costs.get(cell_type, 999.0)\n",
        "                    new_cost = cost_so_far[curr] + step_cost\n",
        "\n",
        "                    if step_cost < 100:\n",
        "                        if (nr, nc) not in cost_so_far or new_cost < cost_so_far[(nr, nc)]:\n",
        "                            cost_so_far[(nr, nc)] = new_cost\n",
        "                            priority = new_cost + abs(nr-end_pos[0]) + abs(nc-end_pos[1])\n",
        "                            heapq.heappush(pq, (priority, (nr, nc)))\n",
        "                            came_from[(nr, nc)] = curr\n",
        "\n",
        "        if status_msg == \"Solved\":\n",
        "            curr = end_pos\n",
        "            while curr != start_pos:\n",
        "                path.append(curr)\n",
        "                curr = came_from[curr]\n",
        "            path.append(start_pos)\n",
        "\n",
        "    # 6. Visualization\n",
        "    img_vis = cv2.imread(str(image_path))\n",
        "    img_vis = cv2.resize(img_vis, (400, 400))\n",
        "    cell_h, cell_w = 400 // rows, 400 // cols\n",
        "\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            if mask_tensor[r, c] == 1:\n",
        "                lbl = terrain_map[r, c]\n",
        "                cx, cy = int((c + 0.5) * cell_w), int((r + 0.5) * cell_h)\n",
        "                color = (100, 100, 100)\n",
        "                if \"Start\" in lbl: color = (0, 255, 255) # Yellow\n",
        "                if \"End\" in lbl: color = (0, 0, 255)     # Red\n",
        "                if any(x in lbl for x in [\"Obstacle\", \"Wall\", \"Tree\", \"Cacti\", \"Rocks\", \"Plasma\"]): color = (0, 0, 0)\n",
        "                if \"Hazard\" in lbl: color = (0, 165, 255) # Orange\n",
        "                cv2.circle(img_vis, (cx, cy), 3, color, -1)\n",
        "            else:\n",
        "                cx, cy = int((c + 0.5) * cell_w), int((r + 0.5) * cell_h)\n",
        "                cv2.circle(img_vis, (cx, cy), 1, (50, 50, 50), -1)\n",
        "\n",
        "    if path:\n",
        "        for i in range(len(path) - 1):\n",
        "            p1 = (int((path[i][1]+0.5)*cell_w), int((path[i][0]+0.5)*cell_h))\n",
        "            p2 = (int((path[i+1][1]+0.5)*cell_w), int((path[i+1][0]+0.5)*cell_h))\n",
        "            cv2.line(img_vis, p1, p2, (0, 255, 0), 2)\n",
        "\n",
        "    return img_vis, f\"{dominant_biome}: {status_msg}\"\n",
        "\n",
        "# ==========================================\n",
        "# 5. EXECUTION\n",
        "# ==========================================\n",
        "test_files = list(TEST_IMG_DIR.glob(\"*.png\"))\n",
        "samples = random.sample(test_files, min(len(test_files), NUM_SAMPLES))\n",
        "\n",
        "print(f\"Processing {len(samples)} images...\")\n",
        "results = []\n",
        "for img_path in tqdm(samples):\n",
        "    res_img, status = process_single_image(img_path)\n",
        "    if res_img is not None: results.append((res_img, status))\n",
        "\n",
        "rows = (len(results) + 4) // 5\n",
        "fig, axes = plt.subplots(rows, 5, figsize=(20, 4 * rows))\n",
        "axes = axes.flatten()\n",
        "for i, (img, status) in enumerate(results):\n",
        "    ax = axes[i]\n",
        "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    title_color = 'green' if \"Solved\" in status else 'red'\n",
        "    ax.set_title(status, color=title_color, fontsize=10, fontweight='bold')\n",
        "    ax.axis('off')\n",
        "for j in range(i + 1, len(axes)): axes[j].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-21T08:22:14.507389Z",
          "iopub.execute_input": "2025-12-21T08:22:14.507736Z",
          "iopub.status.idle": "2025-12-21T08:24:58.590027Z",
          "shell.execute_reply.started": "2025-12-21T08:22:14.507708Z",
          "shell.execute_reply": "2025-12-21T08:24:58.588595Z"
        },
        "id": "CtHUwW-cqMBo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import json\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION\n",
        "# ==========================================\n",
        "DATA_DIR = Path(\"/kaggle/working/generated_dataset_final_v5\")\n",
        "MODEL_SAVE_PATH = \"terrain_classifier_resnet.pth\"\n",
        "MAPPING_SAVE_PATH = \"class_mapping.json\"\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.0001\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. DATA LOADING & RESNET SETUP\n",
        "# ==========================================\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "full_dataset = datasets.ImageFolder(root=DATA_DIR, transform=transform)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"🚀 Loading ResNet-34...\")\n",
        "model = models.resnet34(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(full_dataset.classes))\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# ==========================================\n",
        "# 3. TRAINING LOOP WITH LOGGING\n",
        "# ==========================================\n",
        "# Lists to store metrics\n",
        "history = {\n",
        "    'train_loss': [], 'val_loss': [],\n",
        "    'train_acc': [], 'val_acc': []\n",
        "}\n",
        "\n",
        "print(f\"Starting Training on {len(full_dataset.classes)} classes...\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    # --- TRAINING PHASE ---\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
        "    for images, labels in loop:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100 * correct_train / total_train\n",
        "\n",
        "    # --- VALIDATION PHASE ---\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_acc = 100 * correct_val / total_val\n",
        "\n",
        "    # Store Stats\n",
        "    history['train_loss'].append(avg_train_loss)\n",
        "    history['val_loss'].append(avg_val_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f} | Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. SAVE & VISUALIZE\n",
        "# ==========================================\n",
        "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "idx_to_class = {v: k for k, v in full_dataset.class_to_idx.items()}\n",
        "with open(MAPPING_SAVE_PATH, 'w') as f:\n",
        "    json.dump(idx_to_class, f)\n",
        "print(f\"✅ Saved Model to {MODEL_SAVE_PATH}\")\n",
        "\n",
        "# PLOTTING\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Loss Side by Side\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_loss'], label='Train Loss', color='blue', linestyle='--')\n",
        "plt.plot(history['val_loss'], label='Val Loss', color='red')\n",
        "plt.title('Loss: Training vs Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 2: Accuracy Side by Side\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['train_acc'], label='Train Acc', color='blue', linestyle='--')\n",
        "plt.plot(history['val_acc'], label='Val Acc', color='green')\n",
        "plt.title('Accuracy: Training vs Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-21T07:21:29.858511Z",
          "iopub.execute_input": "2025-12-21T07:21:29.858833Z",
          "iopub.status.idle": "2025-12-21T08:14:05.076702Z",
          "shell.execute_reply.started": "2025-12-21T07:21:29.858807Z",
          "shell.execute_reply": "2025-12-21T08:14:05.075599Z"
        },
        "id": "4vebrwH3qMBr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import heapq\n",
        "import json\n",
        "from pathlib import Path\n",
        "from sklearn.cluster import DBSCAN\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION\n",
        "# ==========================================\n",
        "MODEL_PATH = \"/kaggle/working/terrain_classifier_granular7.pth\"\n",
        "MAPPING_PATH = \"class_mapping.json\"\n",
        "TEST_IMG_DIR = Path(\"/kaggle/input/the-blind-flight-synapse-drive-ps-1/SynapseDrive_Dataset/test/images\")\n",
        "CELL_SIZE = 64\n",
        "MAX_ROWS = 20\n",
        "MAX_COLS = 20\n",
        "NUM_SAMPLES = 50\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Cost Tables (V3 - Split Classes)\n",
        "COST_TABLES = {\n",
        "    \"Desert\": {\n",
        "        \"Road\": 1.2, \"Start\": 1.2, \"End\": 2.2, \"Hazard\": 3.7, \"Cacti\": 999.0, \"Rocks\": 999.0, \"Obstacle\": 999.0, \"Unknown\": 8.0\n",
        "    },\n",
        "    \"Forest\": {\n",
        "        \"Road\": 1.5, \"Start\": 1.5, \"End\": 2.5, \"Hazard\": 2.8, \"Tree\": 999.0, \"Obstacle\": 999.0, \"Unknown\": 8.0\n",
        "    },\n",
        "    \"Lab\": {\n",
        "        \"Road\": 1.0, \"Start\": 1.0, \"End\": 2.0, \"Hazard\": 3.0, \"Wall\": 999.0, \"Plasma\": 999.0, \"Obstacle\": 999.0, \"Unknown\": 8.0\n",
        "    }\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# 2. CORE HELPERS (Raw Vision - No Tint Fix)\n",
        "# ==========================================\n",
        "def preprocess_for_grid(img):\n",
        "    # Removed correct_white_balance!\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    no_texture = cv2.medianBlur(gray, 7)\n",
        "    thresh = cv2.adaptiveThreshold(no_texture, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                   cv2.THRESH_BINARY_INV, 19, 5)\n",
        "    return thresh\n",
        "\n",
        "def get_intersections(img):\n",
        "    thresh = preprocess_for_grid(img)\n",
        "    scale = 25\n",
        "    h_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (scale, 1))\n",
        "    v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, scale))\n",
        "    mask_h = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, h_kernel)\n",
        "    mask_v = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, v_kernel)\n",
        "    intersections = cv2.bitwise_and(mask_h, mask_v)\n",
        "    intersections = cv2.dilate(intersections, np.ones((5,5)))\n",
        "    num_labels, _, stats, centroids = cv2.connectedComponentsWithStats(intersections)\n",
        "    points = [centroids[i] for i in range(1, num_labels) if stats[i, cv2.CC_STAT_AREA] > 10]\n",
        "    if not points: return np.array([])\n",
        "    clustering = DBSCAN(eps=20, min_samples=1).fit(points)\n",
        "    points = np.array(points)\n",
        "    clean_points = [np.mean(points[clustering.labels_ == label], axis=0) for label in set(clustering.labels_)]\n",
        "    return np.array(clean_points)\n",
        "\n",
        "def sort_points_robust(points):\n",
        "    y_clustering = DBSCAN(eps=25, min_samples=3).fit(points[:, 1].reshape(-1, 1))\n",
        "    rows_dict = {}\n",
        "    for pt, label in zip(points, y_clustering.labels_):\n",
        "        if label == -1: continue\n",
        "        if label not in rows_dict: rows_dict[label] = []\n",
        "        rows_dict[label].append(pt)\n",
        "    sorted_keys = sorted(rows_dict.keys(), key=lambda k: np.mean([p[1] for p in rows_dict[k]]))\n",
        "    return [np.array(sorted(rows_dict[k], key=lambda p: p[0])) for k in sorted_keys]\n",
        "\n",
        "def get_global_column_grid(grid_rows):\n",
        "    all_x = [pt[0] for row in grid_rows for pt in row]\n",
        "    if not all_x: return np.array([])\n",
        "    all_x = np.array(all_x).reshape(-1, 1)\n",
        "    clustering = DBSCAN(eps=15, min_samples=1).fit(all_x)\n",
        "    col_centers = [np.mean(all_x[clustering.labels_ == label]) for label in set(clustering.labels_) if label != -1]\n",
        "    return np.array(sorted(col_centers))\n",
        "\n",
        "# ==========================================\n",
        "# 3. ROBUST EXTRACTOR\n",
        "# ==========================================\n",
        "def extract_grid_data(image_path):\n",
        "    img = cv2.imread(str(image_path))\n",
        "    if img is None: return None, None, \"Load Error\"\n",
        "\n",
        "    grid_tensor = np.zeros((MAX_ROWS, MAX_COLS, CELL_SIZE, CELL_SIZE, 3), dtype=np.uint8)\n",
        "    mask_tensor = np.zeros((MAX_ROWS, MAX_COLS), dtype=np.uint8)\n",
        "\n",
        "    try:\n",
        "        points = get_intersections(img)\n",
        "        if len(points) < 40: return grid_tensor, mask_tensor, \"Fallback: Low Pts\"\n",
        "        grid_rows = sort_points_robust(points)\n",
        "        if len(grid_rows) < 4: return grid_tensor, mask_tensor, \"Fallback: Few Rows\"\n",
        "        col_centers = get_global_column_grid(grid_rows)\n",
        "        if len(col_centers) < 2: return grid_tensor, mask_tensor, \"Fallback: No Cols\"\n",
        "\n",
        "        for r in range(min(len(grid_rows) - 1, MAX_ROWS)):\n",
        "            row_top = grid_rows[r]\n",
        "            row_btm = grid_rows[r+1]\n",
        "            for pt_top in row_top:\n",
        "                neighbors_tr = [p for p in row_top if p[0] > pt_top[0]]\n",
        "                if not neighbors_tr: continue\n",
        "                pt_tr = min(neighbors_tr, key=lambda p: p[0])\n",
        "                if abs(pt_tr[0] - pt_top[0]) > 100: continue\n",
        "\n",
        "                candidates_bl = [p for p in row_btm if abs(p[0] - pt_top[0]) < 70]\n",
        "                if not candidates_bl: continue\n",
        "                pt_bl = min(candidates_bl, key=lambda p: abs(p[0] - pt_top[0]))\n",
        "                candidates_br = [p for p in row_btm if abs(p[0] - pt_tr[0]) < 70]\n",
        "                if not candidates_br: continue\n",
        "                pt_br = min(candidates_br, key=lambda p: abs(p[0] - pt_tr[0]))\n",
        "\n",
        "                src = np.array([pt_top, pt_tr, pt_br, pt_bl], dtype=\"float32\")\n",
        "                dst = np.array([[0,0], [CELL_SIZE,0], [CELL_SIZE,CELL_SIZE], [0,CELL_SIZE]], dtype=\"float32\")\n",
        "                M = cv2.getPerspectiveTransform(src, dst)\n",
        "\n",
        "                # Warping pure image (No tint fix applied)\n",
        "                warped = cv2.warpPerspective(img, M, (CELL_SIZE, CELL_SIZE))\n",
        "\n",
        "                diffs = np.abs(col_centers - pt_top[0])\n",
        "                c_idx = np.argmin(diffs)\n",
        "                if c_idx < MAX_COLS:\n",
        "                    grid_tensor[r, c_idx] = warped\n",
        "                    mask_tensor[r, c_idx] = 1\n",
        "        return grid_tensor, mask_tensor, \"Active: Gridded\"\n",
        "    except: return grid_tensor, mask_tensor, \"Error\"\n",
        "\n",
        "# ==========================================\n",
        "# 4. MODEL LOADING\n",
        "# ==========================================\n",
        "class SimpleTerrainCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleTerrainCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "with open(MAPPING_PATH, 'r') as f:\n",
        "    idx_to_class = json.load(f)\n",
        "    idx_to_class = {int(k): v for k, v in idx_to_class.items()}\n",
        "\n",
        "model = SimpleTerrainCNN(num_classes=len(idx_to_class)).to(device)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# ==========================================\n",
        "# 5. BATCH PROCESSOR (Weighted Biome Voting)\n",
        "# ==========================================\n",
        "def process_single_image(image_path):\n",
        "    grid_tensor, mask_tensor, status = extract_grid_data(image_path)\n",
        "    if \"Active\" not in status: return None, \"Slicer Fail\"\n",
        "\n",
        "    rows, cols = mask_tensor.shape\n",
        "    batch_tensors = []\n",
        "    coords = []\n",
        "\n",
        "    transform_pipe = transforms.Compose([\n",
        "        transforms.ToPILImage(), transforms.Resize((64, 64)), transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            if mask_tensor[r, c] == 1:\n",
        "                rgb_cell = cv2.cvtColor(grid_tensor[r, c], cv2.COLOR_BGR2RGB)\n",
        "                batch_tensors.append(transform_pipe(rgb_cell))\n",
        "                coords.append((r, c))\n",
        "\n",
        "    if not batch_tensors: return None, \"Empty Grid\"\n",
        "\n",
        "    batch_stack = torch.stack(batch_tensors).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(batch_stack)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "    terrain_map = np.full((rows, cols), \"Unknown\", dtype=object)\n",
        "\n",
        "    # --- WEIGHTED BIOME VOTING ---\n",
        "    # Give higher weight to unique items (Cacti/Plasma) to override generic Roads\n",
        "    counts = {\"Desert\": 0, \"Forest\": 0, \"Lab\": 0}\n",
        "\n",
        "    for i, (r, c) in enumerate(coords):\n",
        "        class_name = idx_to_class[preds[i].item()]\n",
        "        terrain_map[r, c] = class_name\n",
        "\n",
        "        biome = class_name.split(\"_\")[0] if \"_\" in class_name else \"Lab\"\n",
        "\n",
        "        weight = 1\n",
        "        # If it's a unique object, it counts for 3 votes\n",
        "        if any(x in class_name for x in [\"Cacti\", \"Tree\", \"Plasma\", \"Wall\", \"Rocks\"]):\n",
        "            weight = 3\n",
        "        if \"Start\" in class_name or \"End\" in class_name:\n",
        "            weight = 2\n",
        "\n",
        "        counts[biome] = counts.get(biome, 0) + weight\n",
        "\n",
        "    dominant_biome = max(counts, key=counts.get) if counts else \"Lab\"\n",
        "    costs = COST_TABLES.get(dominant_biome, COST_TABLES[\"Lab\"])\n",
        "\n",
        "    # 6. Solver\n",
        "    start_pos, end_pos = None, None\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            if \"Start\" in terrain_map[r, c]: start_pos = (r, c)\n",
        "            if \"End\" in terrain_map[r, c]: end_pos = (r, c)\n",
        "\n",
        "    path = []\n",
        "    status_msg = \"No Path\"\n",
        "\n",
        "    if start_pos and end_pos:\n",
        "        pq = [(0, start_pos)]\n",
        "        cost_so_far = {start_pos: 0}\n",
        "        came_from = {}\n",
        "\n",
        "        while pq:\n",
        "            curr_cost, curr = heapq.heappop(pq)\n",
        "            if curr == end_pos:\n",
        "                status_msg = \"Solved\"\n",
        "                break\n",
        "\n",
        "            r, c = curr\n",
        "            for nr, nc in [(r-1,c), (r+1,c), (r,c-1), (r,c+1)]:\n",
        "                if 0 <= nr < rows and 0 <= nc < cols:\n",
        "                    label = terrain_map[nr, nc]\n",
        "\n",
        "                    if label == \"Unknown\": cell_type = \"Unknown\"\n",
        "                    else: cell_type = label.split(\"_\")[1] if \"_\" in label else label\n",
        "\n",
        "                    step_cost = costs.get(cell_type, 999.0)\n",
        "                    new_cost = cost_so_far[curr] + step_cost\n",
        "\n",
        "                    if step_cost < 100:\n",
        "                        if (nr, nc) not in cost_so_far or new_cost < cost_so_far[(nr, nc)]:\n",
        "                            cost_so_far[(nr, nc)] = new_cost\n",
        "                            # Heuristic: simple Manhattan distance\n",
        "                            priority = new_cost + abs(nr-end_pos[0]) + abs(nc-end_pos[1])\n",
        "                            heapq.heappush(pq, (priority, (nr, nc)))\n",
        "                            came_from[(nr, nc)] = curr\n",
        "\n",
        "        if status_msg == \"Solved\":\n",
        "            curr = end_pos\n",
        "            while curr != start_pos:\n",
        "                path.append(curr)\n",
        "                curr = came_from[curr]\n",
        "            path.append(start_pos)\n",
        "\n",
        "    # 7. Visualization\n",
        "    img_vis = cv2.imread(str(image_path))\n",
        "    # No white balance correction here either!\n",
        "    img_vis = cv2.resize(img_vis, (400, 400))\n",
        "    cell_h, cell_w = 400 // rows, 400 // cols\n",
        "\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            if mask_tensor[r, c] == 1:\n",
        "                lbl = terrain_map[r, c]\n",
        "                cx, cy = int((c + 0.5) * cell_w), int((r + 0.5) * cell_h)\n",
        "                color = (100, 100, 100)\n",
        "                if \"Start\" in lbl: color = (0, 255, 255)\n",
        "                if \"End\" in lbl: color = (0, 0, 255)\n",
        "                # Black for obstacles\n",
        "                if any(x in lbl for x in [\"Obstacle\", \"Wall\", \"Tree\", \"Cacti\", \"Rocks\", \"Plasma\"]):\n",
        "                    color = (0, 0, 0)\n",
        "                # Orange for passable hazards\n",
        "                if \"Hazard\" in lbl: color = (0, 165, 255)\n",
        "\n",
        "                cv2.circle(img_vis, (cx, cy), 3, color, -1)\n",
        "            else:\n",
        "                cx, cy = int((c + 0.5) * cell_w), int((r + 0.5) * cell_h)\n",
        "                cv2.circle(img_vis, (cx, cy), 1, (50, 50, 50), -1)\n",
        "\n",
        "    if path:\n",
        "        for i in range(len(path) - 1):\n",
        "            p1 = (int((path[i][1]+0.5)*cell_w), int((path[i][0]+0.5)*cell_h))\n",
        "            p2 = (int((path[i+1][1]+0.5)*cell_w), int((path[i+1][0]+0.5)*cell_h))\n",
        "            cv2.line(img_vis, p1, p2, (0, 255, 0), 2)\n",
        "\n",
        "    return img_vis, f\"{dominant_biome}: {status_msg}\"\n",
        "\n",
        "# ==========================================\n",
        "# 6. RUN\n",
        "# ==========================================\n",
        "test_files = list(TEST_IMG_DIR.glob(\"*.png\"))\n",
        "samples = random.sample(test_files, min(len(test_files), NUM_SAMPLES))\n",
        "\n",
        "print(f\"Processing {len(samples)} images...\")\n",
        "results = []\n",
        "for img_path in tqdm(samples):\n",
        "    res_img, status = process_single_image(img_path)\n",
        "    if res_img is not None: results.append((res_img, status))\n",
        "\n",
        "rows = (len(results) + 4) // 5\n",
        "fig, axes = plt.subplots(rows, 5, figsize=(20, 4 * rows))\n",
        "axes = axes.flatten()\n",
        "for i, (img, status) in enumerate(results):\n",
        "    ax = axes[i]\n",
        "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    title_color = 'green' if \"Solved\" in status else 'red'\n",
        "    ax.set_title(status, color=title_color, fontsize=10, fontweight='bold')\n",
        "    ax.axis('off')\n",
        "for j in range(i + 1, len(axes)): axes[j].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-21T08:59:30.375495Z",
          "iopub.execute_input": "2025-12-21T08:59:30.375829Z",
          "iopub.status.idle": "2025-12-21T09:00:21.247445Z",
          "shell.execute_reply.started": "2025-12-21T08:59:30.375803Z",
          "shell.execute_reply": "2025-12-21T09:00:21.2458Z"
        },
        "id": "fLOiCupxqMBr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, models\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import heapq\n",
        "import json\n",
        "from pathlib import Path\n",
        "from sklearn.cluster import DBSCAN\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION\n",
        "# ==========================================\n",
        "# Paths to your TWO best models\n",
        "PATH_SIMPLE_CNN = \"terrain_classifier_granular7.pth\" # Model A (Good for Lab/Forest)\n",
        "PATH_RESNET     = \"terrain_classifier_resnet.pth\"    # Model B (Good for Desert)\n",
        "MAPPING_PATH    = \"class_mapping.json\"\n",
        "\n",
        "TEST_IMG_DIR = Path(\"/kaggle/input/the-blind-flight-synapse-drive-ps-1/SynapseDrive_Dataset/test/images\")\n",
        "CELL_SIZE = 64\n",
        "MAX_ROWS = 20\n",
        "MAX_COLS = 20\n",
        "NUM_SAMPLES = 50\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Unified Cost Table\n",
        "COST_TABLES = {\n",
        "    \"Desert\": {\"Road\": 1.2, \"Start\": 1.2, \"End\": 2.2, \"Hazard\": 3.7, \"Cacti\": 999.0, \"Rocks\": 999.0, \"Obstacle\": 999.0, \"Unknown\": 8.0},\n",
        "    \"Forest\": {\"Road\": 1.5, \"Start\": 1.5, \"End\": 2.5, \"Hazard\": 2.8, \"Tree\": 999.0, \"Obstacle\": 999.0, \"Unknown\": 8.0},\n",
        "    \"Lab\":    {\"Road\": 1.0, \"Start\": 1.0, \"End\": 2.0, \"Hazard\": 5.0, \"Wall\": 999.0, \"Plasma\": 999.0, \"Obstacle\": 999.0, \"Unknown\": 8.0}\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# 2. DEFINE BOTH ARCHITECTURES\n",
        "# ==========================================\n",
        "class SimpleTerrainCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleTerrainCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# ==========================================\n",
        "# 3. LOAD BOTH MODELS\n",
        "# ==========================================\n",
        "with open(MAPPING_PATH, 'r') as f:\n",
        "    idx_to_class = json.load(f)\n",
        "    idx_to_class = {int(k): v for k, v in idx_to_class.items()}\n",
        "num_classes = len(idx_to_class)\n",
        "\n",
        "print(\"🚀 Loading Hybrid Engine...\")\n",
        "\n",
        "# --- Load Model A (Simple CNN) ---\n",
        "model_a = SimpleTerrainCNN(num_classes).to(device)\n",
        "try:\n",
        "    model_a.load_state_dict(torch.load(PATH_SIMPLE_CNN, map_location=device))\n",
        "    print(\"✅ Model A (SimpleCNN) Loaded\")\n",
        "except:\n",
        "    print(\"⚠️ Model A not found or mismatch. Check path.\")\n",
        "\n",
        "# --- Load Model B (ResNet) ---\n",
        "model_b = models.resnet34(pretrained=False) # Or resnet18 depending on what you trained last\n",
        "num_ftrs = model_b.fc.in_features\n",
        "model_b.fc = nn.Linear(num_ftrs, num_classes)\n",
        "try:\n",
        "    model_b.load_state_dict(torch.load(PATH_RESNET, map_location=device))\n",
        "    print(\"✅ Model B (ResNet) Loaded\")\n",
        "except:\n",
        "    print(\"⚠️ Model B not found or mismatch. Check path.\")\n",
        "\n",
        "model_a.eval()\n",
        "model_b.eval()\n",
        "\n",
        "# --- Define Transforms for each ---\n",
        "# SimpleCNN usually expects raw tensors\n",
        "transform_a = transforms.Compose([\n",
        "    transforms.ToPILImage(), transforms.Resize((64, 64)), transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# ResNet expects Normalization\n",
        "transform_b = transforms.Compose([\n",
        "    transforms.ToPILImage(), transforms.Resize((64, 64)), transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ==========================================\n",
        "# 4. VISION HELPERS (Grid Extraction)\n",
        "# ==========================================\n",
        "def preprocess_for_grid(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    no_texture = cv2.medianBlur(gray, 7)\n",
        "    thresh = cv2.adaptiveThreshold(no_texture, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                   cv2.THRESH_BINARY_INV, 19, 5)\n",
        "    return thresh\n",
        "\n",
        "def get_intersections(img):\n",
        "    thresh = preprocess_for_grid(img)\n",
        "    scale = 25\n",
        "    h_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (scale, 1))\n",
        "    v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, scale))\n",
        "    mask_h = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, h_kernel)\n",
        "    mask_v = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, v_kernel)\n",
        "    intersections = cv2.bitwise_and(mask_h, mask_v)\n",
        "    intersections = cv2.dilate(intersections, np.ones((5,5)))\n",
        "    num_labels, _, stats, centroids = cv2.connectedComponentsWithStats(intersections)\n",
        "    points = [centroids[i] for i in range(1, num_labels) if stats[i, cv2.CC_STAT_AREA] > 10]\n",
        "    if not points: return np.array([])\n",
        "    clustering = DBSCAN(eps=20, min_samples=1).fit(points)\n",
        "    points = np.array(points)\n",
        "    clean_points = [np.mean(points[clustering.labels_ == label], axis=0) for label in set(clustering.labels_)]\n",
        "    return np.array(clean_points)\n",
        "\n",
        "def sort_points_robust(points):\n",
        "    y_clustering = DBSCAN(eps=25, min_samples=3).fit(points[:, 1].reshape(-1, 1))\n",
        "    rows_dict = {}\n",
        "    for pt, label in zip(points, y_clustering.labels_):\n",
        "        if label == -1: continue\n",
        "        if label not in rows_dict: rows_dict[label] = []\n",
        "        rows_dict[label].append(pt)\n",
        "    sorted_keys = sorted(rows_dict.keys(), key=lambda k: np.mean([p[1] for p in rows_dict[k]]))\n",
        "    return [np.array(sorted(rows_dict[k], key=lambda p: p[0])) for k in sorted_keys]\n",
        "\n",
        "def get_global_column_grid(grid_rows):\n",
        "    all_x = [pt[0] for row in grid_rows for pt in row]\n",
        "    if not all_x: return np.array([])\n",
        "    all_x = np.array(all_x).reshape(-1, 1)\n",
        "    clustering = DBSCAN(eps=15, min_samples=1).fit(all_x)\n",
        "    col_centers = [np.mean(all_x[clustering.labels_ == label]) for label in set(clustering.labels_) if label != -1]\n",
        "    return np.array(sorted(col_centers))\n",
        "\n",
        "def extract_grid_data(image_path):\n",
        "    img = cv2.imread(str(image_path))\n",
        "    if img is None: return None, None, \"Load Error\"\n",
        "\n",
        "    grid_tensor = np.zeros((MAX_ROWS, MAX_COLS, CELL_SIZE, CELL_SIZE, 3), dtype=np.uint8)\n",
        "    mask_tensor = np.zeros((MAX_ROWS, MAX_COLS), dtype=np.uint8)\n",
        "\n",
        "    try:\n",
        "        points = get_intersections(img)\n",
        "        if len(points) < 40: return grid_tensor, mask_tensor, \"Fallback: Low Pts\"\n",
        "        grid_rows = sort_points_robust(points)\n",
        "        if len(grid_rows) < 4: return grid_tensor, mask_tensor, \"Fallback: Few Rows\"\n",
        "        col_centers = get_global_column_grid(grid_rows)\n",
        "        if len(col_centers) < 2: return grid_tensor, mask_tensor, \"Fallback: No Cols\"\n",
        "\n",
        "        for r in range(min(len(grid_rows) - 1, MAX_ROWS)):\n",
        "            row_top = grid_rows[r]\n",
        "            row_btm = grid_rows[r+1]\n",
        "            for pt_top in row_top:\n",
        "                neighbors_tr = [p for p in row_top if p[0] > pt_top[0]]\n",
        "                if not neighbors_tr: continue\n",
        "                pt_tr = min(neighbors_tr, key=lambda p: p[0])\n",
        "                if abs(pt_tr[0] - pt_top[0]) > 100: continue\n",
        "                candidates_bl = [p for p in row_btm if abs(p[0] - pt_top[0]) < 70]\n",
        "                if not candidates_bl: continue\n",
        "                pt_bl = min(candidates_bl, key=lambda p: abs(p[0] - pt_top[0]))\n",
        "                candidates_br = [p for p in row_btm if abs(p[0] - pt_tr[0]) < 70]\n",
        "                if not candidates_br: continue\n",
        "                pt_br = min(candidates_br, key=lambda p: abs(p[0] - pt_tr[0]))\n",
        "\n",
        "                src = np.array([pt_top, pt_tr, pt_br, pt_bl], dtype=\"float32\")\n",
        "                dst = np.array([[0,0], [CELL_SIZE,0], [CELL_SIZE,CELL_SIZE], [0,CELL_SIZE]], dtype=\"float32\")\n",
        "                M = cv2.getPerspectiveTransform(src, dst)\n",
        "                warped = cv2.warpPerspective(img, M, (CELL_SIZE, CELL_SIZE))\n",
        "\n",
        "                diffs = np.abs(col_centers - pt_top[0])\n",
        "                c_idx = np.argmin(diffs)\n",
        "                if c_idx < MAX_COLS:\n",
        "                    grid_tensor[r, c_idx] = warped\n",
        "                    mask_tensor[r, c_idx] = 1\n",
        "        return grid_tensor, mask_tensor, \"Active: Gridded\"\n",
        "    except: return grid_tensor, mask_tensor, \"Error\"\n",
        "\n",
        "# ==========================================\n",
        "# 5. THE ROUTER (Determine Desert vs Non-Desert)\n",
        "# ==========================================\n",
        "def detect_desert_mode(image_path):\n",
        "    \"\"\"\n",
        "    Returns True if the image is likely Desert (Yellow/Red dominant).\n",
        "    Returns False if Lab/Forest (Green/Blue/Grey dominant).\n",
        "    \"\"\"\n",
        "    img = cv2.imread(str(image_path))\n",
        "    # Resize to 1x1 to get average color\n",
        "    avg_color = cv2.resize(img, (1, 1)).reshape(3)\n",
        "    b, g, r = avg_color\n",
        "\n",
        "    # Desert Logic: Red & Green (Yellow) are significantly higher than Blue\n",
        "    # Lab/Forest Logic: Blue is present, or Green is dominant without Red\n",
        "\n",
        "    is_desert = False\n",
        "\n",
        "    # Simple check: Is Red dominant? (Sand is yellowish-red)\n",
        "    if r > b + 20 and g > b + 10:\n",
        "        is_desert = True\n",
        "\n",
        "    return is_desert\n",
        "\n",
        "# ==========================================\n",
        "# 6. HYBRID BATCH PROCESSOR\n",
        "# ==========================================\n",
        "def process_single_image(image_path):\n",
        "    # --- ROUTING STEP ---\n",
        "    is_desert = detect_desert_mode(image_path)\n",
        "\n",
        "    if is_desert:\n",
        "        active_model = model_b  # ResNet\n",
        "        active_transform = transform_b\n",
        "        model_name = \"RESNET (Desert)\"\n",
        "    else:\n",
        "        active_model = model_a  # SimpleCNN\n",
        "        active_transform = transform_a\n",
        "        model_name = \"SimpleCNN (Std)\"\n",
        "\n",
        "    # 1. Slice\n",
        "    grid_tensor, mask_tensor, status = extract_grid_data(image_path)\n",
        "    if \"Active\" not in status: return None, \"Slicer Fail\"\n",
        "\n",
        "    rows, cols = mask_tensor.shape\n",
        "    batch_tensors = []\n",
        "    coords = []\n",
        "\n",
        "    # 2. Prepare Batch\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            if mask_tensor[r, c] == 1:\n",
        "                # BGR -> RGB\n",
        "                rgb_cell = cv2.cvtColor(grid_tensor[r, c], cv2.COLOR_BGR2RGB)\n",
        "                # Apply Model-Specific Transform\n",
        "                batch_tensors.append(active_transform(rgb_cell))\n",
        "                coords.append((r, c))\n",
        "\n",
        "    if not batch_tensors: return None, \"Empty Grid\"\n",
        "\n",
        "    # 3. Predict\n",
        "    batch_stack = torch.stack(batch_tensors).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = active_model(batch_stack)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "    # 4. Build Map\n",
        "    terrain_map = np.full((rows, cols), \"Unknown\", dtype=object)\n",
        "    counts = {\"Desert\": 0, \"Forest\": 0, \"Lab\": 0}\n",
        "\n",
        "    for i, (r, c) in enumerate(coords):\n",
        "        class_name = idx_to_class[preds[i].item()]\n",
        "        terrain_map[r, c] = class_name\n",
        "\n",
        "        biome = class_name.split(\"_\")[0] if \"_\" in class_name else \"Lab\"\n",
        "\n",
        "        weight = 1\n",
        "        if any(x in class_name for x in [\"Cacti\", \"Tree\", \"Plasma\", \"Wall\", \"Rocks\"]): weight = 3\n",
        "        if \"Start\" in class_name or \"End\" in class_name: weight = 2\n",
        "\n",
        "        counts[biome] = counts.get(biome, 0) + weight\n",
        "\n",
        "    dominant_biome = max(counts, key=counts.get) if counts else \"Lab\"\n",
        "    costs = COST_TABLES.get(dominant_biome, COST_TABLES[\"Lab\"])\n",
        "\n",
        "    # 5. Solver (Standard)\n",
        "    start_pos, end_pos = None, None\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            if \"Start\" in terrain_map[r, c]: start_pos = (r, c)\n",
        "            if \"End\" in terrain_map[r, c]: end_pos = (r, c)\n",
        "\n",
        "    path = []\n",
        "    status_msg = \"No Path\"\n",
        "\n",
        "    if start_pos and end_pos:\n",
        "        pq = [(0, start_pos)]\n",
        "        cost_so_far = {start_pos: 0}\n",
        "        came_from = {}\n",
        "\n",
        "        while pq:\n",
        "            curr_cost, curr = heapq.heappop(pq)\n",
        "            if curr == end_pos:\n",
        "                status_msg = \"Solved\"\n",
        "                break\n",
        "\n",
        "            r, c = curr\n",
        "            for nr, nc in [(r-1,c), (r+1,c), (r,c-1), (r,c+1)]:\n",
        "                if 0 <= nr < rows and 0 <= nc < cols:\n",
        "                    label = terrain_map[nr, nc]\n",
        "\n",
        "                    if label == \"Unknown\": cell_type = \"Unknown\"\n",
        "                    else: cell_type = label.split(\"_\")[1] if \"_\" in label else label\n",
        "\n",
        "                    step_cost = costs.get(cell_type, 999.0)\n",
        "                    new_cost = cost_so_far[curr] + step_cost\n",
        "\n",
        "                    if step_cost < 100:\n",
        "                        if (nr, nc) not in cost_so_far or new_cost < cost_so_far[(nr, nc)]:\n",
        "                            cost_so_far[(nr, nc)] = new_cost\n",
        "                            priority = new_cost + abs(nr-end_pos[0]) + abs(nc-end_pos[1])\n",
        "                            heapq.heappush(pq, (priority, (nr, nc)))\n",
        "                            came_from[(nr, nc)] = curr\n",
        "\n",
        "        if status_msg == \"Solved\":\n",
        "            curr = end_pos\n",
        "            while curr != start_pos:\n",
        "                path.append(curr)\n",
        "                curr = came_from[curr]\n",
        "            path.append(start_pos)\n",
        "\n",
        "    # 6. Visualization\n",
        "    img_vis = cv2.imread(str(image_path))\n",
        "    img_vis = cv2.resize(img_vis, (400, 400))\n",
        "    cell_h, cell_w = 400 // rows, 400 // cols\n",
        "\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            if mask_tensor[r, c] == 1:\n",
        "                lbl = terrain_map[r, c]\n",
        "                cx, cy = int((c + 0.5) * cell_w), int((r + 0.5) * cell_h)\n",
        "                color = (100, 100, 100)\n",
        "                if \"Start\" in lbl: color = (0, 255, 255)\n",
        "                if \"End\" in lbl: color = (0, 0, 255)\n",
        "                if any(x in lbl for x in [\"Obstacle\", \"Wall\", \"Tree\", \"Cacti\", \"Rocks\", \"Plasma\"]): color = (0, 0, 0)\n",
        "                if \"Hazard\" in lbl: color = (0, 165, 255)\n",
        "                cv2.circle(img_vis, (cx, cy), 3, color, -1)\n",
        "            else:\n",
        "                cx, cy = int((c + 0.5) * cell_w), int((r + 0.5) * cell_h)\n",
        "                cv2.circle(img_vis, (cx, cy), 1, (50, 50, 50), -1)\n",
        "\n",
        "    if path:\n",
        "        for i in range(len(path) - 1):\n",
        "            p1 = (int((path[i][1]+0.5)*cell_w), int((path[i][0]+0.5)*cell_h))\n",
        "            p2 = (int((path[i+1][1]+0.5)*cell_w), int((path[i+1][0]+0.5)*cell_h))\n",
        "            cv2.line(img_vis, p1, p2, (0, 255, 0), 2)\n",
        "\n",
        "    # Add Model Name to Status for Debugging\n",
        "    return img_vis, f\"{model_name}\\n{dominant_biome}: {status_msg}\"\n",
        "\n",
        "# ==========================================\n",
        "# 7. RUN\n",
        "# ==========================================\n",
        "test_files = list(TEST_IMG_DIR.glob(\"*.png\"))\n",
        "samples = random.sample(test_files, min(len(test_files), NUM_SAMPLES))\n",
        "\n",
        "print(f\"Processing {len(samples)} images...\")\n",
        "results = []\n",
        "for img_path in tqdm(samples):\n",
        "    res_img, status = process_single_image(img_path)\n",
        "    if res_img is not None: results.append((res_img, status))\n",
        "\n",
        "rows = (len(results) + 4) // 5\n",
        "fig, axes = plt.subplots(rows, 5, figsize=(20, 4 * rows))\n",
        "axes = axes.flatten()\n",
        "for i, (img, status) in enumerate(results):\n",
        "    ax = axes[i]\n",
        "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    title_color = 'green' if \"Solved\" in status else 'red'\n",
        "    ax.set_title(status, color=title_color, fontsize=9, fontweight='bold')\n",
        "    ax.axis('off')\n",
        "for j in range(i + 1, len(axes)): axes[j].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-21T09:07:30.705964Z",
          "iopub.execute_input": "2025-12-21T09:07:30.706966Z"
        },
        "id": "wKCIJhO-qMBs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "a35FD3vyqMBu"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}